# SocraticDev - Complete Requirements Document

## 1. Project Overview

### 1.1 Vision

SocraticDev is an AI-powered code learning platform that revolutionizes how developers learn programming by combining the ancient Socratic teaching method with cutting-edge code intelligence technology. Unlike traditional learning platforms that simply provide answers, SocraticDev guides learners to discover solutions through carefully crafted questions, building deep understanding rather than surface-level knowledge. The platform integrates GraphRAG (Graph Retrieval-Augmented Generation) technology to understand entire codebase structures, enabling context-aware AI assistance that references actual code relationships and dependencies. This creates a learning environment where developers don't just memorize syntax—they understand architectural patterns, design decisions, and the interconnected nature of software systems.

### 1.2 Target Users

**Computer Science Students**: University and college students taking programming courses who need to understand not just how to write code, but why code works the way it does. These users benefit from the Socratic method's emphasis on conceptual understanding and the ability to explore real codebases to see theory in practice.

**Bootcamp Learners**: Individuals in intensive coding bootcamps who need to rapidly build both practical skills and theoretical understanding. The platform's challenge system (The Dojo) provides deliberate practice opportunities, while the spaced repetition system ensures long-term retention of concepts learned during the compressed bootcamp timeline.

**Junior Developers**: Early-career developers (0-2 years experience) who are transitioning from learning to professional work. They need to understand existing codebases quickly, learn best practices, and develop debugging skills. The GraphRAG system helps them navigate unfamiliar code, while the AI tutor guides them through problem-solving without simply providing copy-paste solutions.

**Senior Developers**: Experienced developers who need context-aware code assistance for large codebases or want to explore new technologies. The Building Mode provides direct, production-ready solutions when needed, while the code analysis features help understand complex dependency chains and impact of changes.

### 1.3 Core Value Proposition

**Learn by Thinking**: The platform implements the Socratic method where the AI asks 1-3 guiding questions before providing answers. This forces learners to engage their critical thinking skills, make predictions, and reason through problems. Research shows that active recall and generation (thinking through problems) creates stronger neural pathways than passive reading. By making users think before showing solutions, SocraticDev builds problem-solving skills that transfer to new situations, not just memorized patterns.

**Context-Aware Intelligence**: Traditional AI coding assistants lack understanding of your specific codebase structure. SocraticDev's GraphRAG system parses your entire project, storing code entities (functions, classes, variables) in a graph database (Neo4j) and generating semantic embeddings (Chroma vector database). When you ask a question, the system performs hybrid search—combining semantic similarity (finding conceptually related code) with graph traversal (finding structurally connected code)—to inject relevant context into AI prompts. This means the AI understands your authentication flow, knows which functions call each other, and can explain how changes will ripple through your system.

**Deliberate Practice**: The Dojo provides 10 different challenge types, each targeting specific skills: Parsons Problems (code ordering), Code Surgery (bug finding), ELI5 (explanation), Fill the Blanks (completion), Mental Compiler (output prediction), Rubber Duck (debugging), Code Translation (language conversion), TDD (test-driven), Pattern Detective (design patterns), and Big O Battle (complexity analysis). This variety ensures comprehensive skill development across reading, writing, debugging, and analyzing code. Each challenge type is grounded in computer science education research on effective learning strategies.

**Long-term Retention**: The Spaced Repetition System (SRS) implements the SM-2 algorithm, scientifically proven to optimize memory retention. As you learn concepts, the system automatically generates flashcards and schedules reviews at increasing intervals (1 day, 6 days, then exponentially). Cards you struggle with appear more frequently, while mastered concepts appear less often. This ensures knowledge moves from short-term to long-term memory, preventing the common problem of "learning" something only to forget it weeks later.

**Engaging Experience**: Gamification elements (XP, leagues, achievements, daily quests) provide extrinsic motivation while the intrinsic satisfaction of understanding drives long-term engagement. The system tracks progress across six skill dimensions (algorithms, data structures, debugging, design patterns, testing, architecture), visualized in a skill radar chart. Users see tangible progress, unlock achievements for milestones, and compete in leagues (Bronze → Silver → Gold → Platinum → Diamond) based on total XP earned. This game-like progression maintains motivation during the challenging process of learning to code.



### 1.4 Technology Stack

**Frontend Technologies:**
- **React 18.3.1**: Latest React version with concurrent rendering features for smooth UI updates, automatic batching for better performance, and improved Suspense for data fetching. The concurrent features allow the UI to remain responsive even during heavy computations like parsing large code files or rendering complex graphs.
- **TypeScript 5.3.3**: Provides static type checking to catch errors at compile time rather than runtime, improving code quality and developer experience. Type inference reduces boilerplate while maintaining safety. Strict mode enabled for maximum type safety.
- **Vite 5.1.0**: Next-generation build tool that provides instant server start (no bundling in dev), lightning-fast Hot Module Replacement (HMR), and optimized production builds. Uses native ES modules in development for speed, then Rollup for production bundling.
- **Tailwind CSS 3.4.1**: Utility-first CSS framework that enables rapid UI development without writing custom CSS. Provides consistent design system through configuration, automatic purging of unused styles for small bundle sizes, and responsive design utilities.
- **GSAP 3.14.2**: Professional-grade animation library for complex, performant animations on the landing page. Provides timeline-based sequencing, scroll-triggered animations, and 60fps performance through GPU acceleration.
- **Framer Motion 12.26.2**: React animation library for component animations, providing declarative API for animations, gestures, and layout animations. Handles animation orchestration, variants for complex sequences, and automatic cleanup.
- **Zustand 4.5.0**: Lightweight state management (1KB) with simple API, no provider wrapper needed, excellent TypeScript support, and middleware for persistence. Chosen over Redux for simplicity and over Context API for performance.
- **Monaco Editor 4.6.0**: VS Code's editor component, providing syntax highlighting, IntelliSense, code completion, and multi-language support. Used in code challenges and visualizer for professional editing experience.
- **ReactFlow 11.11.4**: Library for building node-based graphs and diagrams. Used for dependency graph visualization, call graph display, and interactive code structure exploration. Supports custom nodes, edges, and layouts.

**Backend Technologies:**
- **FastAPI 0.104.1**: Modern Python web framework with automatic API documentation (OpenAPI/Swagger), native async support for high concurrency, Pydantic integration for request/response validation, and excellent performance (comparable to Node.js and Go).
- **Python 3.11+**: Latest Python with performance improvements (10-60% faster than 3.10), better error messages, and modern type hinting features. Chosen for excellent data science/ML library ecosystem and readability.
- **Neo4j 5.14.1**: Graph database for storing code structure and relationships. Provides Cypher query language for intuitive graph traversal, ACID transactions, and optimized algorithms for path finding, centrality, and community detection. Perfect for representing code dependencies.
- **Chroma 0.4.15**: Open-source vector database for semantic search. Stores 768-dimensional embeddings with metadata, provides fast similarity search using HNSW algorithm, and supports filtering. Self-hostable alternative to Pinecone/Weaviate.
- **Redis 5.0.1**: In-memory data store used for caching query results (5-minute TTL), session management, and rate limiting. Provides sub-millisecond latency for cached data, reducing database load and improving response times.
- **Celery 5.3.4**: Distributed task queue for asynchronous processing. Handles long-running operations like project uploads (parsing 10,000 files takes minutes), embedding generation, and batch operations. Supports retries, scheduling, and monitoring.
- **Tree-sitter 0.21.3**: Incremental parsing library that builds concrete syntax trees from source code. Supports multiple languages (Python, JavaScript, TypeScript, Java, C++, C#), handles syntax errors gracefully, and provides fast, accurate parsing for code analysis.
- **Google Gemini API 0.3.1**: Google's latest AI model for chat completions and embeddings. Gemini 2.0 Flash provides fast responses with good reasoning, while text-embedding-004 generates 768-dimensional vectors for semantic search. Chosen for performance and cost-effectiveness.



## 2. Functional Requirements

### 2.1 User Authentication and Profile Management

**FR-2.1.1: Email/Password Registration**
The system shall provide a registration form where users can create accounts using email and password. The password must meet security requirements: minimum 8 characters, at least one uppercase letter, one lowercase letter, one number, and one special character. The system shall hash passwords using bcrypt with a cost factor of 12 before storing in the database. Email addresses must be validated for correct format and uniqueness. Upon successful registration, the system shall send a verification email with a time-limited token (24 hours) to confirm email ownership. Users cannot access full features until email is verified. This prevents spam accounts and ensures users can recover their accounts.

**FR-2.1.2: OAuth Authentication**
The system shall support OAuth 2.0 authentication with Google and GitHub as identity providers. When a user clicks "Sign in with Google" or "Sign in with GitHub", the system shall redirect to the provider's authorization page, request appropriate scopes (email, profile), and handle the callback with authorization code. The system shall exchange the code for an access token, retrieve user profile information, and either create a new account or link to an existing account based on email address. OAuth provides better security (no password storage), improved user experience (one-click login), and leverages existing trusted identity providers.

**FR-2.1.3: User Profile Management**
The system shall maintain user profiles containing: display name, email, avatar URL, learning preferences (preferred programming languages, difficulty level, daily goal), notification settings (email notifications for achievements, daily reminders), and privacy settings (profile visibility, leaderboard participation). Users shall be able to update their profile information through a settings page. Changes to email address require re-verification. The profile serves as the central hub for personalization and allows the system to tailor content to individual learning styles and goals.

**FR-2.1.4: Theme Switching**
The system shall provide a theme toggle allowing users to switch between light and dark modes. The toggle shall be accessible from the navigation bar on all pages. When toggled, the system shall immediately apply the new theme by adding/removing a 'dark' class on the document root element, triggering CSS custom property changes. The selected theme shall be persisted in localStorage under the key 'socraticdev-theme' so it persists across sessions and browser restarts. The system shall also respect the user's system preference (prefers-color-scheme media query) as the default before any manual selection. Dark mode reduces eye strain during extended coding sessions and is preferred by many developers.

**FR-2.1.5: Preference Persistence**
The system shall persist user preferences in localStorage for immediate availability without server round-trips. Preferences include: theme (light/dark), AI mode (learning/building), sidebar state (open/closed), code editor settings (font size, tab size, theme), and recently accessed projects. Each preference shall be stored under a namespaced key (e.g., 'socraticdev-editor-font-size') to avoid conflicts with other applications. The system shall load these preferences on application initialization and apply them before rendering the UI to prevent visual flashing. For authenticated users, preferences shall also sync to the server for cross-device consistency.

### 2.2 AI Chat Interface

**FR-2.2.1: Dual AI Modes**
The system shall provide two distinct AI interaction modes accessible via a prominent toggle in the chat interface. Learning Mode emphasizes the Socratic method: the AI asks 1-3 guiding questions before providing answers, encourages users to make predictions, and breaks down complex problems into smaller conceptual steps. The system prompt instructs the AI to use question types like "What do you think will happen if...?", "How does this relate to...?", and "Can you explain why...?". Building Mode provides direct, production-ready solutions with brief explanations, focusing on efficiency for experienced developers who need quick answers. The mode selection affects the system prompt sent to the AI, the temperature parameter (0.8 for Learning, 0.7 for Building), and the response formatting.

**FR-2.2.2: Socratic Questioning in Learning Mode**
When in Learning Mode, the AI shall implement the Socratic method by asking 1-3 guiding questions before providing direct answers. These questions shall be designed to activate prior knowledge, encourage prediction, and build conceptual understanding. For example, if a user asks "How do I implement binary search?", the AI might respond: "Great question! Before we dive into the implementation, let's think through the concept: 1) What property must the data have for binary search to work? 2) How does binary search differ from linear search in terms of the search space? 3) What happens to the search space after each comparison?" Only after the user engages with these questions does the AI provide the implementation. This approach, backed by cognitive science research, creates stronger learning than passive information transfer.

**FR-2.2.3: Direct Solutions in Building Mode**
When in Building Mode, the AI shall provide direct, production-ready code solutions with minimal preamble. Responses shall include: complete, runnable code with proper error handling, type hints/annotations, edge case handling, brief explanation of key decisions, and suggestions for testing. For example, if asked "How do I implement binary search?", the AI immediately provides a complete, well-documented implementation with time/space complexity analysis. This mode is optimized for experienced developers who understand the concepts and need efficient implementation guidance. The system prompt emphasizes code quality, best practices, and production readiness.

**FR-2.2.4: Conversation History Management**
The system shall maintain conversation history for the current session, storing messages in memory with the structure: {role: 'user'|'assistant', content: string, timestamp: number}. The chat interface shall display messages in chronological order with visual distinction between user and assistant messages (different background colors, alignment, avatars). Users shall be able to scroll through history, and the system shall automatically scroll to the latest message when new messages arrive. The conversation history is included in subsequent AI requests to maintain context (last 10 messages or 4000 tokens, whichever is smaller). History is cleared on page refresh or when starting a new conversation, as persistent conversation storage is not implemented in the current version.

**FR-2.2.5: Markdown Rendering**
The system shall render AI responses as formatted markdown, supporting: headers (h1-h6), bold/italic text, lists (ordered/unordered), links, blockquotes, tables, and inline code. The markdown parser shall sanitize HTML to prevent XSS attacks while allowing safe markdown syntax. This enables the AI to structure responses clearly with sections, emphasize key points, and provide well-formatted explanations. For example, the AI can create comparison tables, step-by-step numbered lists, or highlight important concepts in bold.



**FR-2.2.6: Syntax Highlighting for Code Blocks**
The system shall automatically detect and syntax highlight code blocks in AI responses using PrismJS or similar library. Code blocks shall be identified by triple backtick markdown syntax with optional language identifier (```python, ```javascript, etc.). The system shall support syntax highlighting for all major languages: Python, JavaScript, TypeScript, Java, C++, C#, Go, Rust, SQL, HTML, CSS, JSON, YAML, and Bash. Highlighted code shall use a theme consistent with the application's light/dark mode. Syntax highlighting improves code readability by visually distinguishing keywords, strings, comments, and other language elements, making it easier for users to understand code examples provided by the AI.

**FR-2.2.7: Code Snippet Copy Functionality**
The system shall provide a "Copy" button in the top-right corner of each code block in AI responses. When clicked, the button shall copy the code to the user's clipboard using the Clipboard API (navigator.clipboard.writeText), provide visual feedback by changing the button text to "Copied!" for 2 seconds, and handle errors gracefully if clipboard access is denied. This feature eliminates the need for manual text selection and copying, which can be error-prone with formatted code. Users can quickly copy code examples to their editor for testing or modification, significantly improving the workflow when learning from AI-generated examples.

**FR-2.2.8: Project Context Integration**
When a user has uploaded a project, the system shall automatically inject relevant code context into AI prompts. The context retrieval process works as follows: (1) User submits a question, (2) System generates embedding for the question using Gemini API, (3) System performs hybrid search combining semantic similarity (Chroma vector search) and structural relevance (Neo4j graph traversal), (4) System ranks results by combined relevance score (70% semantic, 30% structural), (5) System selects top entities within token budget (8000 tokens), (6) System formats context with source citations and injects into prompt. This enables the AI to provide answers specific to the user's codebase, referencing actual functions, classes, and patterns from their project rather than generic examples.

**FR-2.2.9: Learning Mode AI Configuration**
The system shall configure the Gemini API with specific parameters for Learning Mode: temperature=0.8 (higher creativity and exploration), top_k=40, top_p=0.95, max_output_tokens=8192. The system prompt shall emphasize Socratic questioning with instructions like: "You are a Socratic tutor. Before providing direct answers, ask 1-3 guiding questions that help the user discover the solution themselves. Use questions that activate prior knowledge, encourage prediction, and build conceptual understanding. After the user engages with your questions, provide clear explanations with examples." The higher temperature encourages more varied and exploratory responses, suitable for the open-ended nature of learning conversations.

**FR-2.2.10: Building Mode AI Configuration**
The system shall configure the Gemini API with specific parameters for Building Mode: temperature=0.7 (balanced between creativity and consistency), top_k=40, top_p=0.95, max_output_tokens=8192. The system prompt shall emphasize direct solutions with instructions like: "You are an expert programming assistant. Provide direct, production-ready code solutions with brief explanations. Focus on: type safety, error handling, edge cases, best practices, and performance. Include comments for complex logic. Be concise but complete." The slightly lower temperature produces more consistent, focused responses appropriate for developers who need reliable, production-quality code.

### 2.3 Project Upload and Code Analysis

**FR-2.3.1: Multiple Upload Methods**
The system shall support three methods for project upload: (1) Drag-and-drop: Users can drag a folder or zip file onto a designated drop zone, which highlights on dragover and triggers upload on drop. (2) File selection: Users can click a button to open the native file picker, select multiple files or a folder (using webkitdirectory attribute), and upload selected files. (3) GitHub URL: Users can paste a GitHub repository URL (e.g., https://github.com/user/repo), and the system clones the repository server-side using git clone or GitHub API. All three methods shall validate file types (only code files), check total file count (max 10,000), and create an upload session for tracking progress. Multiple upload methods accommodate different user preferences and workflows.

**FR-2.3.2: File Count Validation**
The system shall enforce a maximum limit of 10,000 files per project upload to prevent resource exhaustion and ensure reasonable processing times. When a user attempts to upload more than 10,000 files, the system shall reject the upload immediately (before any processing begins), display an error message explaining the limit, and suggest strategies for reducing file count (exclude node_modules, .git, build artifacts, etc.). The validation shall occur on both client-side (for immediate feedback) and server-side (for security). This limit is based on performance testing showing that 10,000 files can be processed in approximately 100 minutes (100 files/minute target), which is acceptable for asynchronous processing.

**FR-2.3.3: Multi-Language Code Parsing**
The system shall parse source code files in six programming languages using Tree-sitter parsers: Python (.py), JavaScript (.js, .jsx), TypeScript (.ts, .tsx), Java (.java), C++ (.cpp, .hpp, .h), and C# (.cs). For each file, the system shall: (1) Detect language from file extension, (2) Load appropriate Tree-sitter parser, (3) Parse file content into Abstract Syntax Tree (AST), (4) Handle syntax errors gracefully by continuing to parse valid portions, (5) Extract code entities from AST, (6) Extract relationships between entities. Each language has specific extraction rules (e.g., Python extracts decorators, JavaScript extracts arrow functions, Java extracts interfaces). Multi-language support enables the platform to analyze diverse codebases and polyglot projects.

**FR-2.3.4: Code Entity Extraction**
The system shall extract six types of code entities from parsed ASTs: (1) Files: Represent source files with path, language, and line count. (2) Functions: Extract name, signature (parameters and return type), docstring/comments, body (first 500 characters), start/end line numbers, and metadata (async, decorators). (3) Classes: Extract name, methods list, inheritance relationships, docstring, start/end line numbers. (4) Methods: Extract as functions but with parent class reference. (5) Variables: Extract module-level or class-level variables with name, type (if annotated), and scope. (6) Imports: Extract module name and imported symbols. Each entity is assigned a unique ID (project_id + entity_type + name + line_number) and stored with metadata for later retrieval.

**FR-2.3.5: Code Relationship Extraction**
The system shall extract six types of relationships between code entities: (1) CALLS: Function A calls Function B, extracted by analyzing function call expressions in AST. (2) IMPORTS: File A imports File/Module B, extracted from import statements. (3) DEFINES: File A defines Function/Class/Variable B, representing containment. (4) EXTENDS: Class A extends Class B, extracted from class inheritance declarations. (5) IMPLEMENTS: Class A implements Interface B (Java, TypeScript), extracted from interface implementations. (6) USES: Function A uses Variable B, extracted from variable references in function body. These relationships form the graph structure that enables dependency analysis, impact analysis, and context retrieval.



**FR-2.3.6: Asynchronous Upload Processing**
The system shall process project uploads asynchronously using Celery distributed task queue to prevent blocking the API and provide better user experience. The workflow is: (1) User initiates upload → (2) System validates files and creates upload session with unique session_id → (3) System returns session_id immediately to user → (4) System queues Celery task with session_id and file paths → (5) Celery worker processes files in background → (6) User polls /api/upload/status/{session_id} for progress updates → (7) System updates session status (pending → processing → completed/failed). This architecture allows users to navigate away during upload, supports concurrent uploads from multiple users, and prevents API timeouts for large projects. The task can be retried if it fails and can be monitored through Celery's flower dashboard.

**FR-2.3.7: Real-Time Progress Updates**
The system shall provide real-time progress updates during upload processing through a polling mechanism. The upload session object shall track: status (pending/processing/completed/failed), progress percentage (0-100), files_processed count, total_files count, entities_extracted count, current_file being processed, and errors list. The frontend shall poll the status endpoint every 2 seconds while status is 'processing', displaying a progress bar, current file name, and statistics. When processing completes, the system shall display final statistics: total files processed, total entities extracted (functions, classes, etc.), total relationships created, processing time, and any errors encountered. This transparency helps users understand what's happening and builds trust in the system.

**FR-2.3.8: Processing Performance Target**
The system shall process uploaded files at a minimum rate of 100 files per minute to ensure reasonable completion times for large projects. This target is achieved through: (1) Parallel processing using multiple Celery workers (4-8 workers recommended), (2) Efficient Tree-sitter parsing (typically 10-50ms per file), (3) Batch database operations (insert 100 entities at once rather than one-by-one), (4) Batch embedding generation (generate 10 embeddings per API call), (5) Connection pooling for database connections. For a 10,000 file project, this target means completion in approximately 100 minutes. The system shall log processing metrics (files/minute, average parse time, database write time) to identify bottlenecks and optimize performance.

**FR-2.3.9: Graceful Error Handling**
The system shall handle individual file parsing errors gracefully without stopping the entire upload process. When a file fails to parse (syntax errors, unsupported language features, corrupted files), the system shall: (1) Log the error with file path, line number, and error message, (2) Continue processing remaining files, (3) Add error to session errors list, (4) Mark file as 'partially parsed' if some entities were extracted, (5) Include all errors in final upload report. The final report shall categorize errors by type (syntax errors, unsupported features, timeouts) and provide actionable suggestions (fix syntax, update parser, exclude file). This approach maximizes the value extracted from uploads even when some files have issues, which is common in real-world codebases.

**FR-2.3.10: Upload Statistics Display**
Upon upload completion, the system shall display comprehensive statistics to help users understand their codebase: (1) Total files processed (by language breakdown), (2) Total entities extracted (functions: X, classes: Y, variables: Z, imports: W), (3) Total relationships created (CALLS: X, IMPORTS: Y, DEFINES: Z, etc.), (4) Lines of code analyzed, (5) Processing time and rate (files/minute), (6) Top 10 most connected entities (functions with most callers/dependencies), (7) Language distribution (pie chart), (8) Error summary (if any). These statistics provide immediate insights into codebase structure and help users verify the upload was successful. The statistics are also stored in the project metadata for later reference.

### 2.4 GraphRAG System - Graph Database

**FR-2.4.1: Entity Storage in Neo4j**
The system shall store all extracted code entities as nodes in Neo4j graph database with appropriate labels and properties. Each entity type has a specific label: File, Function, Class, Variable, Import. Node properties include: id (unique identifier), project_id (for multi-project isolation), name, file_path, start_line, end_line, signature (for functions/methods), docstring, body (truncated to 500 chars), language, and metadata (JSON object for additional properties like decorators, async flag, etc.). The system shall use parameterized Cypher queries to prevent injection attacks and batch operations (UNWIND) to insert multiple nodes efficiently. Nodes are indexed on frequently queried properties (name, project_id, file_path) for fast retrieval.

**FR-2.4.2: Relationship Storage in Neo4j**
The system shall store all extracted code relationships as edges in Neo4j graph database with appropriate relationship types and properties. Relationship types include: CALLS, IMPORTS, DEFINES, EXTENDS, IMPLEMENTS, USES. Each relationship has properties: source_id (entity that initiates relationship), target_id (entity that receives relationship), and metadata (JSON object for additional context like call location, import alias, etc.). The system shall create relationships using Cypher MATCH + CREATE pattern to ensure both source and target nodes exist before creating the edge. For IMPORTS relationships to external modules (not in project), the system shall create ExternalModule nodes to represent third-party dependencies.

**FR-2.4.3: Database Indexing**
The system shall create database indexes on frequently queried properties to optimize query performance. Indexes shall be created for: (1) Function.name - for finding functions by name, (2) Class.name - for finding classes by name, (3) File.path - for finding files by path, (4) Function.project_id - for filtering by project, (5) Class.project_id - for filtering by project, (6) File.project_id - for filtering by project. These indexes are created using Cypher CREATE INDEX commands during application initialization. Indexes significantly improve query performance (10-100x faster) for large graphs by allowing the database to quickly locate nodes without scanning all nodes. The system shall verify indexes exist on startup and create them if missing.

**FR-2.4.4: Function Caller Query**
The system shall support finding all callers of a specific function through graph traversal. The query uses Cypher pattern matching: MATCH (caller)-[:CALLS]->(target {id: $function_id, project_id: $project_id}) WHERE caller.project_id = $project_id RETURN caller. This finds all nodes (functions) that have a CALLS relationship pointing to the target function, filtered by project_id for multi-project isolation. Results include the caller's name, file path, line number, and signature. This query is essential for understanding function usage, identifying dead code (functions with no callers), and assessing impact of changes. The query typically executes in <100ms for graphs with millions of nodes due to indexing.

**FR-2.4.5: Function Dependency Query**
The system shall support finding all dependencies of a specific function through graph traversal. The query uses Cypher pattern matching: MATCH (source {id: $function_id, project_id: $project_id})-[:CALLS|USES]->(dep) WHERE dep.project_id = $project_id RETURN dep. This finds all nodes (functions, variables) that the source function depends on through CALLS or USES relationships. Results include the dependency's name, type, file path, and line number. This query helps developers understand what a function needs to work, identify coupling issues (functions with too many dependencies), and plan refactoring. The query supports multiple relationship types using the pipe operator (|) in Cypher.



**FR-2.4.6: Class Hierarchy Query**
The system shall support querying class inheritance hierarchies through graph traversal in both directions. For parent classes (classes this class extends/implements), the query uses: MATCH (c {id: $class_id})-[:EXTENDS|IMPLEMENTS]->(parent) RETURN parent. For child classes (classes that extend/implement this class), the query uses: MATCH (child)-[:EXTENDS|IMPLEMENTS]->(c {id: $class_id}) RETURN child. The system shall return a ClassHierarchy object containing the root class, list of parent classes, and list of child classes. This enables understanding of inheritance chains, identifying abstract base classes (many children, no parents), and finding leaf classes (no children). The query supports multiple relationship types (EXTENDS for inheritance, IMPLEMENTS for interfaces) which is important for languages like Java and TypeScript.

**FR-2.4.7: Impact Analysis with Depth Limit**
The system shall perform transitive dependency analysis to identify all functions affected by changes to a target function, limited to a maximum depth of 5 to prevent infinite traversal in circular dependencies. The query uses variable-length path matching: MATCH path = (source {id: $function_id})-[:CALLS*1..5]->(dep) WHERE length(path) <= $max_depth RETURN dep, length(path) AS depth, [node IN nodes(path) | node.id] AS path. This finds all functions reachable from the source through CALLS relationships, up to 5 hops away. Results include each dependent function, its depth (distance from source), and the complete path showing how the dependency chain flows. This analysis is crucial for understanding change impact before refactoring and identifying tightly coupled code that may need restructuring.

**FR-2.4.8: Circular Dependency Detection**
The system shall detect circular dependencies during impact analysis by checking if any node appears twice in a dependency path. When traversing the graph, the system collects the node IDs in each path. If len(node_ids) != len(set(node_ids)), a cycle exists. The system shall extract the cycle by finding the repeated node and returning the subpath between its first and second occurrence. For example, if path is [A, B, C, D, B, E], the cycle is [B, C, D, B]. The system shall return all detected cycles in the impact analysis result with a warning message. Circular dependencies indicate design issues (mutual recursion, tight coupling) that can cause stack overflows, make code hard to test, and complicate refactoring. Detecting them helps developers identify and resolve architectural problems.

**FR-2.4.9: Query Performance Target**
The system shall execute graph queries with a 95th percentile response time of less than 500 milliseconds to ensure responsive user experience. This target is achieved through: (1) Database indexes on frequently queried properties, (2) Query optimization (using MATCH instead of WHERE when possible, limiting result sets), (3) Connection pooling to avoid connection overhead, (4) Query result caching in Redis for repeated queries, (5) Query timeouts (5 seconds) to prevent runaway queries. The system shall log slow queries (>1 second) for analysis and optimization. For complex queries like impact analysis on large graphs, the system may use pagination or streaming results to maintain responsiveness while processing large result sets.

**FR-2.4.10: Multi-Project Isolation**
The system shall support multiple projects per user with complete data isolation using project_id namespacing. Every node and relationship includes a project_id property, and all queries filter by project_id to ensure users only see data from their own projects. For example: MATCH (f:Function {project_id: $project_id}) WHERE f.name = $name RETURN f. This prevents data leakage between projects, enables per-project deletion (delete all nodes with specific project_id), and supports project-specific analytics. The system shall validate project ownership before executing queries (user_id matches project.user_id) to prevent unauthorized access. Multi-project support is essential for users working on multiple codebases or teams sharing the platform.

### 2.5 GraphRAG System - Vector Database

**FR-2.5.1: Embedding Generation with Gemini**
The system shall generate 768-dimensional vector embeddings for all functions and classes using Google's Gemini text-embedding-004 model. For each entity, the system constructs an embedding input by concatenating: entity type, name, signature (if function), docstring (if present), and body (first 500 characters). For example: "Function: authenticate_user\nSignature: def authenticate_user(username: str, password: str) -> User\nDocstring: Authenticates user with username and password.\nBody: user = db.query(User).filter_by(username=username).first()...". This rich input captures both the entity's interface (signature) and implementation (body), enabling semantic search to find functionally similar code even with different names. The 768-dimensional vectors are generated by Gemini's transformer model trained on massive code corpora.

**FR-2.5.2: Embedding Storage in Chroma**
The system shall store generated embeddings in Chroma vector database with metadata linking back to Neo4j entities. Each embedding is stored with: id (entity_id matching Neo4j), embedding (768-dimensional float array), and metadata (entity_type, file_path, name, project_id, signature). The system creates one Chroma collection per project (project_{project_id}_embeddings) for isolation and efficient deletion. Chroma uses HNSW (Hierarchical Navigable Small World) algorithm for fast approximate nearest neighbor search, providing sub-second query times even with millions of vectors. The metadata enables filtering search results by entity type, file path, or other criteria without retrieving full entities from Neo4j.

**FR-2.5.3: Semantic Search Across Projects**
The system shall perform semantic similarity search across one or more projects by querying their Chroma collections. The search process: (1) Generate query embedding from user's search text using Gemini, (2) For each project_id in search scope, query its Chroma collection with the query embedding, (3) Retrieve top K results per collection (default K=20), (4) Merge results from all collections, (5) Sort by similarity score (cosine similarity, 0-1 range), (6) Return top 20 overall results. This enables finding similar code patterns across multiple projects, discovering reusable implementations, and learning from past solutions. Cross-project search is particularly valuable for teams with multiple microservices or developers working on related projects.

**FR-2.5.4: Top-K Result Retrieval**
The system shall return the top 20 most similar entities for each semantic search query, balancing comprehensiveness with performance. Returning too few results (e.g., 5) might miss relevant code, while too many (e.g., 100) overwhelms users and increases latency. The top-20 limit is based on user research showing that developers rarely examine more than 20 search results. The system uses Chroma's n_results parameter to limit results at the database level (more efficient than retrieving all and filtering client-side). Results are ranked by cosine similarity score in descending order (most similar first). Users can adjust this limit through an advanced search option if needed for specific use cases.

**FR-2.5.5: Similarity Threshold Filtering**
The system shall filter semantic search results by a minimum similarity threshold (default 0.7) to exclude irrelevant matches. Cosine similarity ranges from 0 (completely dissimilar) to 1 (identical). A threshold of 0.7 means results must be at least 70% similar to the query. This prevents low-quality results from cluttering the interface and wasting user attention. The threshold is configurable per query for flexibility: strict searches (0.8-0.9) for finding near-duplicates, relaxed searches (0.5-0.6) for exploratory browsing. The system displays the similarity score with each result so users can judge relevance. Results below threshold are discarded before returning to the client, reducing payload size and improving performance.



**FR-2.5.6: Search Performance Target**
The system shall execute semantic searches with a 95th percentile response time of less than 1 second to maintain responsive user experience. This target encompasses: embedding generation (200-300ms for query text), Chroma vector search (200-400ms for 1M vectors using HNSW), result merging and sorting (50-100ms), and network overhead (100-200ms). Performance is achieved through: (1) HNSW index in Chroma for O(log n) search complexity, (2) Batch processing when searching multiple projects, (3) Result caching in Redis for repeated queries, (4) Connection pooling to Chroma, (5) Async I/O for concurrent project searches. The system shall log slow searches (>2 seconds) for investigation and optimization. For very large collections (>10M vectors), the system may use approximate search with slightly reduced accuracy for better performance.

**FR-2.5.7: Similar Entity Discovery**
The system shall support finding entities similar to a given entity (e.g., "find functions similar to this one") by using the entity's existing embedding as the query vector. The process: (1) Retrieve entity's embedding from Chroma by entity_id, (2) Use this embedding as query vector (skip embedding generation step), (3) Search Chroma for nearest neighbors, (4) Exclude the source entity from results (it would be the top match with similarity=1.0), (5) Return top K similar entities (default K=10). This feature enables discovering alternative implementations, finding code duplication, identifying refactoring opportunities (multiple similar functions could be consolidated), and learning different approaches to the same problem. It's particularly useful for code review and architectural analysis.

**FR-2.5.8: Project Embedding Deletion**
The system shall support deleting all embeddings for a project when the project is deleted or needs to be re-indexed. The deletion process: (1) Identify Chroma collection name (project_{project_id}_embeddings), (2) Check if collection exists, (3) Get collection count for logging, (4) Delete entire collection using Chroma's delete_collection API, (5) Return count of deleted embeddings. This is more efficient than deleting embeddings one-by-one and ensures complete cleanup. The system shall also delete corresponding Neo4j nodes and relationships in the same transaction to maintain consistency between graph and vector databases. Proper cleanup prevents storage bloat, ensures accurate search results, and allows users to re-upload projects with fresh parsing.

### 2.6 Context Retrieval for AI

**FR-2.6.1: Hybrid Search for Context**
The system shall retrieve relevant code context using a hybrid approach that combines semantic search (vector similarity) and structural search (graph traversal). The process: (1) Generate query embedding from user's question, (2) Perform semantic search in Chroma to find conceptually similar entities (top 20), (3) Perform graph search in Neo4j to find structurally related entities (functions called by/calling entities mentioned in query, classes in same file, etc.), (4) Merge results from both searches, (5) Rank by combined relevance score. This hybrid approach is superior to either method alone: semantic search finds conceptually related code even with different names, while graph search finds structurally connected code that may not be semantically similar but is contextually important (e.g., helper functions, data models).

**FR-2.6.2: Relevance Score Calculation**
The system shall rank context entities by a combined relevance score that balances semantic similarity and structural proximity. The formula: relevance_score = 0.7 * semantic_similarity + 0.3 * (1 / graph_distance). Semantic similarity (0-1) comes from Chroma cosine similarity. Graph distance (1-infinity) is the number of hops in the graph from query-mentioned entities. The 70/30 weighting prioritizes semantic relevance (what the code does) over structural proximity (where it is), based on empirical testing showing this produces better AI responses. For example, a semantically similar function 3 hops away (score = 0.7*0.9 + 0.3*0.33 = 0.73) ranks higher than a less similar function 1 hop away (score = 0.7*0.5 + 0.3*1.0 = 0.65).

**FR-2.6.3: Token Budget Management**
The system shall assemble context within a configurable token budget (default 8000 tokens) to fit within AI model context windows while leaving room for the user's question and AI's response. The assembly process: (1) Sort entities by relevance score (descending), (2) Initialize token count = 0, (3) For each entity in sorted order: estimate tokens (roughly 1 token per 4 characters), if adding entity would exceed budget, stop; otherwise add entity to context and update token count, (4) Format selected entities with source citations. This greedy algorithm maximizes relevance within budget. The 8000 token budget is chosen to fit comfortably in Gemini's 32K context window (8K context + 2K question + 20K response + 2K buffer). Users can adjust the budget for specific queries.

**FR-2.6.4: Entity Prioritization**
When the token budget is exceeded, the system shall prioritize higher-ranked entities and truncate or exclude lower-ranked ones to stay within budget. The prioritization strategy: (1) Always include top 3 entities (most relevant), (2) For remaining entities, include in relevance order until budget reached, (3) If an entity is too large (>1000 tokens), include only its signature and docstring (skip body), (4) Track which entities were included vs. excluded for transparency. This ensures the most important context is always present while gracefully degrading for large codebases. The system shall log when truncation occurs and how many entities were excluded, helping identify queries that need budget adjustment or query refinement.

**FR-2.6.5: Context Formatting with Citations**
The system shall format assembled context with clear source citations to help users understand where information comes from and enable verification. The format: "# Relevant code from your project:\n\n[File: src/auth/login.py, Lines: 15-30]\ndef authenticate_user(username: str, password: str) -> User:\n    \"\"\"Authenticates user with username and password.\"\"\"\n    ...\n\n[File: src/models/user.py, Lines: 5-20]\nclass User:\n    \"\"\"User model with authentication fields.\"\"\"\n    ...". Each code block includes file path and line numbers, enabling users to jump to the source. The formatting uses markdown for readability and syntax highlighting. Citations build trust in AI responses and help users learn where to find information in their codebase.

**FR-2.6.6: Context Usage Display**
The system shall display which code entities were used as context in a dedicated panel or section of the chat interface. For each entity, show: name, type (function/class), file path, line numbers, relevance score, and token count. The display shall also show: total entities included, total token count, remaining token budget, and any entities that were excluded due to budget constraints. Users can click on an entity to view its full code in a modal or side panel. This transparency helps users understand why the AI gave a particular answer, verify the AI used correct context, and learn about their codebase structure. It also enables debugging when AI responses seem incorrect (maybe wrong context was retrieved).



**FR-2.6.7: Manual Context Adjustment**
The system shall allow users to manually add or remove entities from the context before submitting to the AI. The context management panel shall display all automatically selected entities with checkboxes. Users can: (1) Uncheck entities to exclude them (frees up tokens for other entities), (2) Search for and add additional entities not automatically selected, (3) Reorder entities by dragging (affects priority if budget exceeded), (4) View token impact of changes in real-time. This manual control is valuable when users know specific code is relevant but wasn't automatically selected, or when they want to exclude irrelevant entities to make room for more relevant ones. The system shall validate that manual selections don't exceed token budget and warn if they do.

**FR-2.6.8: Context Panel UI Design**
The system shall provide a collapsible context panel in the chat interface showing context details. The panel shall display: (1) Header with total entities, total tokens, and budget remaining, (2) List of entities with: name, type icon, file path (truncated), relevance score (color-coded: green >0.8, yellow 0.6-0.8, orange <0.6), token count, and checkbox for manual selection, (3) Search box to filter entities by name or file path, (4) "View Code" button for each entity to see full source, (5) Warning indicator if budget exceeded with list of excluded entities. The panel shall be collapsible to save screen space when not needed. This UI makes the context retrieval process transparent and gives users control over what the AI sees.

### 2.7 Graph Visualization

**FR-2.7.1: ReactFlow Graph Rendering**
The system shall render code entities and relationships as an interactive graph using ReactFlow library. Each code entity becomes a node with: unique id, position (x, y coordinates), type (determines visual style), label (entity name), and data (full entity object). Each relationship becomes an edge with: id, source node id, target node id, type (determines line style), and label (relationship type). The graph shall use ReactFlow's built-in features: zoom (mouse wheel), pan (click-drag), node selection (click), edge highlighting (hover), and minimap for navigation. The initial layout uses ReactFlow's hierarchical layout algorithm (Dagre) to position nodes based on their relationships, creating a top-down flow from files to functions to dependencies.

**FR-2.7.2: Node Color Coding by Type**
The system shall color-code nodes by entity type for quick visual identification: Files (blue background, #3B82F6), Functions (green background, #10B981), Classes (purple background, #8B5CF6), Variables (orange background, #F59E0B), Imports (gray background, #6B7280). Each node shall also display an icon representing its type (file icon, function icon, class icon, etc.). The color coding follows common conventions (blue for files, green for functions) and provides sufficient contrast for accessibility (WCAG AA compliant). Users can quickly scan the graph and identify different entity types without reading labels, improving comprehension of large graphs.

**FR-2.7.3: Edge Styling by Relationship**
The system shall style edges by relationship type for visual distinction: IMPORTS (dashed line, gray color), CALLS (solid line, blue color), DEFINES (solid line, green color, thicker), EXTENDS (solid line, purple color, thicker), IMPLEMENTS (dashed line, purple color), USES (dotted line, orange color). The line style (solid/dashed/dotted) and thickness convey relationship semantics: solid for strong relationships (calls, defines), dashed for weaker relationships (imports), thicker for structural relationships (defines, extends). Edge labels display the relationship type on hover. This visual language helps users understand code structure at a glance without examining individual relationships.

**FR-2.7.4: Interactive Graph Controls**
The system shall support interactive graph manipulation: (1) Zoom: Mouse wheel or pinch gesture to zoom in/out (10%-500% range), (2) Pan: Click-drag background to move viewport, (3) Node selection: Click node to select (highlights node and connected edges), (4) Multi-select: Ctrl+click to select multiple nodes, (5) Edge highlighting: Hover edge to highlight and show label, (6) Fit view: Button to auto-zoom to fit all nodes in viewport, (7) Reset: Button to reset zoom and pan to initial state. These controls enable exploration of large graphs (1000+ nodes) by focusing on specific areas while maintaining context through the minimap.

**FR-2.7.5: Node Detail Display**
When a user selects a node, the system shall display detailed information in a side panel: (1) Entity name and type, (2) File path with line numbers (clickable to open file), (3) Signature (for functions/methods), (4) Docstring (if present), (5) Code snippet (first 20 lines with syntax highlighting), (6) Statistics: number of callers, number of dependencies, number of lines, (7) Related entities: list of directly connected nodes with relationship types, (8) Actions: "View Full Code", "Find Similar", "Impact Analysis". This detail view provides context for understanding the selected entity without leaving the graph visualization, supporting exploratory code navigation.

**FR-2.7.6: Graph Filtering**
The system shall support filtering the graph by multiple criteria to reduce complexity and focus on specific aspects: (1) Entity type: Show only files, functions, classes, or combinations, (2) Language: Show only entities from specific languages (Python, JavaScript, etc.), (3) File pattern: Show only entities from files matching a regex pattern (e.g., "src/auth/*"), (4) Relationship type: Show only specific relationships (e.g., only CALLS edges), (5) Depth: Show only entities within N hops of a selected node. Filters can be combined (AND logic). The system shall update the graph in real-time as filters change, with smooth animations for node/edge appearance/disappearance. Filtering is essential for working with large codebases where the full graph would be overwhelming.

**FR-2.7.7: Hierarchical Layout for Large Graphs**
When a graph contains more than 500 nodes, the system shall use hierarchical layout algorithm (Dagre) to organize nodes in layers based on their relationships, creating a clear top-down or left-right flow. The algorithm: (1) Identifies root nodes (no incoming edges), (2) Assigns nodes to layers based on longest path from root, (3) Minimizes edge crossings within each layer, (4) Spaces nodes to avoid overlap. For very large graphs (>1000 nodes), the system shall use lazy loading: initially render only nodes within viewport, load additional nodes as user pans/zooms. This maintains 60fps performance even with massive graphs. The system shall also provide alternative layouts: force-directed (organic clustering), circular (shows cycles), and tree (strict hierarchy).

**FR-2.7.8: Node Search and Highlighting**
The system shall provide a search box to find nodes by name, file path, or content. As the user types, the system shall: (1) Filter nodes matching the search query (fuzzy matching), (2) Highlight matching nodes with a distinct color (yellow glow), (3) Dim non-matching nodes (reduced opacity), (4) Display match count ("5 matches"), (5) Provide next/previous buttons to cycle through matches, (6) Center viewport on current match. When a match is selected, the system shall zoom to that node and display its details. Search supports regex patterns for advanced queries (e.g., "auth.*user" to find all auth-related user functions). This feature is crucial for navigating large graphs where visual scanning is impractical.



### 2.8 The Dojo - Challenge System

**FR-2.8.1: Ten Challenge Types**
The system shall provide exactly 10 distinct challenge types, each targeting different cognitive skills required for programming mastery: (1) Parsons Problems - code ordering and logical flow, (2) Code Surgery - bug detection and debugging, (3) ELI5 Mode - explanation and communication, (4) Fill the Blanks - code completion and syntax, (5) Mental Compiler - execution tracing and mental models, (6) Rubber Duck Debugger - problem articulation and debugging, (7) Code Translation - language transfer and paradigm understanding, (8) Test-Driven Development - test-first thinking and specification, (9) Pattern Detective - design pattern recognition and architecture, (10) Big O Battle - complexity analysis and algorithm understanding. This variety ensures comprehensive skill development and prevents monotony. Each challenge type is based on proven pedagogical techniques from computer science education research.

**FR-2.8.2: Parsons Problems Implementation**
The system shall implement Parsons Problems where users drag-and-drop code blocks into the correct order to form a working program. Each challenge consists of: (1) Problem description explaining what the code should do, (2) Scrambled code blocks (5-15 lines each), (3) Distractor blocks (1-3 incorrect lines that shouldn't be used), (4) Correct solution order (hidden from user). The interface uses @dnd-kit library for smooth drag-and-drop with: visual feedback during drag (block follows cursor), drop zones between existing blocks, snap-to-position animation, and undo/redo support. When user submits, the system validates order against solution, highlights incorrect positions in red, and provides hints ("Block 3 should come before Block 5"). Parsons Problems are proven to be more effective than writing code from scratch for learning logical flow, as they reduce cognitive load by removing syntax concerns.

**FR-2.8.3: Code Surgery Implementation**
The system shall implement Code Surgery challenges where users find and fix bugs in provided code. Each challenge contains: (1) Buggy code with 1-5 intentional bugs, (2) Bug types: logic errors (wrong operator, off-by-one), syntax errors (missing semicolon, wrong indentation), security issues (SQL injection, XSS), performance problems (O(n²) when O(n) possible), edge case failures (null handling, empty array), (3) Expected behavior description, (4) Test cases showing failures. Users can: request hints (reveals bug location without solution), run code to see errors, submit fixes for validation. The system validates fixes by: running test cases, checking if bugs are resolved, ensuring no new bugs introduced. AI mode generates bugs dynamically; static mode uses pre-defined buggy code. This challenge type develops critical debugging skills and teaches common pitfall patterns.

**FR-2.8.4: ELI5 Mode Implementation**
The system shall implement ELI5 (Explain Like I'm 5) challenges where users explain complex code in simple terms. Each challenge provides: (1) Code snippet (10-30 lines) implementing a concept (recursion, binary search, etc.), (2) Target audience (5-year-old, non-programmer, junior developer), (3) Forbidden technical jargon list (words that must be avoided), (4) Key points that must be covered. Users write their explanation in a text area. The AI evaluates explanations based on: (1) Clarity - uses simple language and analogies, (2) Accuracy - correctly explains what the code does, (3) Completeness - covers all key points, (4) Avoidance - doesn't use forbidden jargon. The AI provides feedback: "Good use of the 'sorting books' analogy! However, you used the term 'iteration' which might be too technical. Try 'going through each item one by one'." This challenge develops communication skills essential for code review, documentation, and teaching others.

**FR-2.8.5: Fill the Blanks Implementation**
The system shall implement Fill the Blanks (Faded Examples) challenges where users complete partially-written code by filling in missing parts. Each challenge contains: (1) Code template with blanks marked as [___], (2) Hints for each blank (data type, purpose, example), (3) Blank types: variable names, operators, function calls, control flow keywords, (4) Progressive difficulty (early challenges have more code provided, later ones have more blanks). The interface provides: input fields at each blank position, autocomplete suggestions based on context, syntax highlighting, real-time validation (red border for syntax errors). When submitted, the system validates each blank: exact match for keywords/operators, semantic match for variable names (accepts synonyms), functional match for expressions (accepts equivalent code). This challenge type implements the "fading" technique where scaffolding gradually decreases as learners gain competence.

**FR-2.8.6: Mental Compiler Implementation**
The system shall implement Mental Compiler challenges where users predict code output without running it, developing mental execution models. Each challenge provides: (1) Code snippet (10-20 lines), (2) Initial variable values, (3) Multiple choice options for output (4-5 options including common mistakes), (4) Step-by-step execution trace (revealed after submission). Users select the predicted output. After submission, the system: (1) Shows correct answer, (2) Displays execution trace with variable states at each line, (3) Explains why wrong answers are incorrect, (4) Highlights the line where user's mental model diverged from actual execution. Challenge types include: loops (counting iterations), conditionals (branch prediction), recursion (call stack visualization), variable mutation (tracking state changes). This develops the crucial skill of "running code in your head" which expert programmers use constantly.

**FR-2.8.7: Rubber Duck Debugger Implementation**
The system shall implement Rubber Duck Debugger challenges where users explain their code to an AI "duck" that asks clarifying questions, simulating the rubber duck debugging technique. The process: (1) User pastes buggy code or describes a problem, (2) AI duck asks questions: "What do you expect this line to do?", "What is the value of X at this point?", "Why did you use this approach?", (3) User answers questions, (4) Through answering, user often realizes the bug themselves, (5) If not, AI provides gentle hints based on answers. The AI is programmed to be curious but not judgmental, asking genuine questions rather than leading ones. This challenge teaches the metacognitive skill of articulating assumptions and reasoning, which often reveals flaws in logic. It's based on the real-world practice where explaining code to someone (or a rubber duck) helps find bugs.

**FR-2.8.8: Code Translation Implementation**
The system shall implement Code Translation challenges where users translate code between programming languages, developing language-agnostic understanding of algorithms. Supported translations: Python ↔ JavaScript ↔ TypeScript ↔ Java (12 translation pairs). Each challenge provides: (1) Source code in one language, (2) Target language specification, (3) Requirements (maintain functionality, use idiomatic patterns, handle type differences). Users write the translation in a code editor with syntax highlighting. The system validates translations by: (1) Running test cases in target language, (2) Comparing outputs with source code outputs, (3) Checking for idiomatic patterns (e.g., list comprehensions in Python, map/filter in JavaScript), (4) AI evaluation of code quality and style. This challenge develops understanding of language paradigms (imperative vs. functional), type systems (dynamic vs. static), and standard libraries, making developers more versatile.

**FR-2.8.9: Test-Driven Development Implementation**
The system shall implement TDD challenges where users write code to pass given test cases, learning test-first development. Each challenge provides: (1) Problem description, (2) Test suite with 5-10 test cases (unit tests), (3) Empty function/class template, (4) Expected behavior for each test. Users write implementation in code editor. The system: (1) Runs tests after each submission, (2) Shows which tests pass (green) and fail (red), (3) Displays failure messages and expected vs. actual values, (4) Tracks test coverage (which lines executed), (5) Provides hints if stuck ("Test 3 fails because your code doesn't handle negative numbers"). Users must pass all tests to complete challenge. This teaches the TDD cycle (red-green-refactor), writing testable code, and thinking about edge cases before implementation. It's a fundamental professional development practice.

**FR-2.8.10: Pattern Detective Implementation**
The system shall implement Pattern Detective challenges where users identify design patterns and code smells in provided code. Each challenge shows: (1) Code snippet implementing a pattern (Singleton, Factory, Observer, Strategy, etc.) or containing a smell (God Class, Long Method, Duplicate Code), (2) Multiple choice options (4-5 patterns/smells), (3) Context about the code's purpose. Users select the pattern/smell. After submission, the system: (1) Shows correct answer, (2) Explains the pattern/smell characteristics, (3) Highlights code sections demonstrating the pattern, (4) Discusses when to use/avoid the pattern, (5) Suggests refactoring for code smells. Advanced challenges show multiple patterns in one codebase or ask users to refactor smells. This develops architectural thinking and code quality awareness essential for senior developers.

**FR-2.8.11: Big O Battle Implementation**
The system shall implement Big O Battle challenges where users identify algorithm time/space complexity under time pressure, developing quick complexity analysis skills. Each challenge shows: (1) Code snippet (algorithm implementation), (2) Multiple choice options for time complexity (O(1), O(log n), O(n), O(n log n), O(n²), O(2ⁿ)), (3) Multiple choice options for space complexity, (4) Time limit (30-60 seconds per question). The interface displays: countdown timer, current score, streak counter. Users must answer quickly and accurately. After each answer, the system: (1) Shows correct complexity, (2) Explains the analysis (loop structure, recursion depth, data structure operations), (3) Shows similar algorithms for comparison. Challenges progress from simple (single loop) to complex (nested loops, recursion, divide-and-conquer). The time pressure simulates interview conditions and builds fluency in complexity analysis, a fundamental skill for algorithm design and optimization.



**FR-2.8.12: Multi-Language Support**
The system shall support challenges in six programming languages: Python, JavaScript, TypeScript, Java, C++, and C#. Each challenge type shall have examples in all six languages, allowing users to practice in their preferred language or learn new languages. The language selection shall be persistent across sessions (stored in localStorage) and affect: (1) Code editor syntax highlighting, (2) Available challenges (language-specific examples), (3) AI-generated content (when AI mode enabled), (4) Code execution environment (for challenges that run code). The system shall use Monaco Editor's language support for syntax highlighting and IntelliSense. Multi-language support makes the platform accessible to diverse learners and enables polyglot developers to practice multiple languages in one place.

**FR-2.8.13: Language Preference Selection**
The system shall provide a language selector in the Dojo hub interface, displayed prominently in the header. The selector shall be a dropdown menu showing all six supported languages with their icons (Python snake, JavaScript JS logo, etc.). When a user changes language, the system shall: (1) Update localStorage ('socraticdev-dojo-language'), (2) Reload challenge examples for the new language, (3) Update code editor language mode, (4) Show a toast notification confirming the change. The selected language shall be used as the default for all new challenges until changed again. This allows users to focus on one language for deep learning or switch languages to compare implementations and broaden their skills.

**FR-2.8.14: AI Mode Toggle**
The system shall provide an AI mode toggle in the Dojo hub, allowing users to choose between static pre-defined challenges and dynamically AI-generated challenges. When AI mode is enabled, the system: (1) Generates challenge content on-demand using Gemini API, (2) Creates unique challenges each time (no repetition), (3) Adapts difficulty based on user performance, (4) Generates bugs for Code Surgery, (5) Creates test cases for TDD, (6) Produces code examples for Pattern Detective. When AI mode is disabled, the system uses curated, hand-crafted challenges from the examples database. AI mode provides infinite variety and personalization but requires API calls (cost, latency). Static mode provides consistent, vetted challenges with instant loading. Users can toggle based on their needs and internet connectivity.

**FR-2.8.15: Challenge Completion Tracking**
The system shall track challenge completion statistics per category, storing data in localStorage under 'socraticdev-dojo-stats'. For each challenge type, track: (1) Total attempts, (2) Successful completions, (3) Success rate (completions / attempts), (4) Average time to complete, (5) Best time, (6) Total points earned, (7) Last attempted date. The Dojo hub shall display these statistics on each challenge card: "Completed 15x" badge, success rate bar chart, personal best time. This tracking provides motivation through visible progress, helps users identify weak areas (low success rate), and enables the system to recommend appropriate difficulty levels. Statistics are synced to server for authenticated users to enable cross-device progress tracking.

**FR-2.8.16: Points and Rewards System**
The system shall award points for challenge completion based on difficulty and performance. Point calculation: Base points (Easy: 10, Medium: 25, Hard: 50) × Performance multiplier (Perfect: 2.0x, Good: 1.5x, Acceptable: 1.0x, Poor: 0.5x) × Speed bonus (Completed in <50% of time limit: +50%, <75%: +25%). For example, a Hard challenge (50 points) completed perfectly (2.0x) in half the time limit (+50%) awards 150 points. Points contribute to: (1) Total XP for league progression, (2) Daily quest completion, (3) Achievement unlocking, (4) Leaderboards (future). The system shall display points earned after each challenge with a celebratory animation and breakdown showing how points were calculated. This gamification element provides immediate feedback and extrinsic motivation.

### 2.9 Spaced Repetition System (SRS)

**FR-2.9.1: SM-2 Algorithm Implementation**
The system shall implement the SuperMemo 2 (SM-2) spaced repetition algorithm for optimal review scheduling. The algorithm maintains three values per card: (1) Interval - days until next review (starts at 1), (2) Repetitions - count of successful reviews (starts at 0), (3) Ease Factor - multiplier for interval growth (starts at 2.5). After each review, the algorithm updates these values based on user's quality rating (0-5): If quality ≥ 3 (correct), interval increases (first review: 1 day, second: 6 days, subsequent: interval × ease_factor); if quality < 3 (incorrect), interval resets to 1 day and repetitions reset to 0. Ease factor adjusts based on difficulty: ease_factor += 0.1 - (5 - quality) × (0.08 + (5 - quality) × 0.02), clamped to minimum 1.3. This algorithm is scientifically proven to optimize long-term retention by scheduling reviews just before forgetting occurs.

**FR-2.9.2: Three Flashcard Types**
The system shall support three flashcard types optimized for different learning scenarios: (1) Basic - traditional front/back cards with question on front, answer on back. Used for definitions, concepts, syntax rules. (2) Cloze - text with one or more blanks marked as [...], user must fill in blanks. Used for code completion, formula memorization, fill-in-the-blank facts. (3) Code - code snippet on front, explanation/output/purpose on back. Used for code reading, pattern recognition, algorithm understanding. Each type has a specific UI: Basic shows front then flips to back, Cloze shows text with input fields for blanks, Code shows syntax-highlighted code with collapsible answer. Users can create any type manually or let the system auto-generate from learning activities.

**FR-2.9.3: Manual Card Creation**
The system shall provide a card creation interface accessible from the SRS dashboard. The interface includes: (1) Type selector (Basic/Cloze/Code), (2) Front content editor (rich text for Basic, text with [...] markers for Cloze, code editor for Code), (3) Back content editor (rich text for Basic/Cloze, explanation editor for Code), (4) Language selector (for Code cards), (5) Tags input (comma-separated, for organization), (6) Source type (manual), (7) Preview button (shows card as it will appear in review). The system validates: front and back are not empty, cloze cards have at least one [...] marker, code cards have valid syntax. Created cards are immediately added to the deck with initial SM-2 values and scheduled for first review. This allows users to create custom cards for any concept they want to remember.

**FR-2.9.4: Auto-Generation from Chat**
The system shall automatically generate flashcards from AI chat conversations when users learn new concepts. The generation process: (1) Monitor chat for learning moments (AI explains a concept, provides a definition, shows an example), (2) Extract key information (concept name, explanation, code example), (3) Create appropriate card type (Basic for definitions, Code for examples), (4) Add to user's deck with source_type='chat' and timestamp, (5) Show notification: "Created flashcard: 'What is binary search?'" with preview and option to edit/delete. Users can disable auto-generation in settings. This passive card creation ensures users build a review deck without manual effort, capturing knowledge as they learn. The system uses heuristics to identify learning moments: AI uses phrases like "In other words", "For example", "The key concept is", or provides code examples.

**FR-2.9.5: Auto-Generation from Dojo**
The system shall automatically generate flashcards from completed Dojo challenges to reinforce learned concepts. After completing a challenge, the system: (1) Identifies key concepts (algorithm used, pattern demonstrated, bug type fixed), (2) Creates cards based on challenge type: Parsons → code ordering card, Code Surgery → bug identification card, ELI5 → explanation card, Pattern Detective → pattern recognition card, (3) Includes challenge code as reference, (4) Adds to deck with source_type='dojo' and challenge_id, (5) Shows summary: "Created 2 flashcards from this challenge" with option to review. This reinforces challenge learning through spaced repetition, ensuring concepts move to long-term memory. Users can disable per-challenge-type in settings (e.g., only create cards from Pattern Detective and TDD challenges).



**FR-2.9.6: SM-2 Review Scheduling**
The system shall schedule card reviews based on SM-2 intervals, ensuring optimal spacing for long-term retention. Each card has a nextReview timestamp (Unix milliseconds). The system calculates due cards by comparing current time with nextReview: cards where nextReview ≤ now are due for review. The review queue is sorted by: (1) Overdue cards first (nextReview oldest first), (2) New cards (never reviewed) second, (3) Due today cards last. This prioritization ensures users don't fall behind on reviews while still introducing new material. The system displays due count prominently: "15 cards due today" with breakdown (5 overdue, 3 new, 7 due). Users can adjust daily review limit (default 50 cards) to manage workload. The scheduling algorithm ensures reviews happen just before forgetting, maximizing retention efficiency.

**FR-2.9.7: Six Quality Ratings**
The system shall support six quality ratings (0-5) as defined by the SM-2 algorithm, allowing precise difficulty feedback: (0) Complete blackout - no recollection, (1) Incorrect, but upon seeing answer, remembered, (2) Incorrect, but answer seemed easy to recall, (3) Correct with serious difficulty, (4) Correct with some hesitation, (5) Perfect response. Each rating has a label, color, and description displayed during review. The rating affects interval calculation: 0-2 reset progress (interval=1, repetitions=0), 3-5 increase interval (3: small increase, 5: large increase). The six-level granularity allows the algorithm to fine-tune scheduling based on actual difficulty, but can be overwhelming for users, so the system also provides a simplified 4-button interface.

**FR-2.9.8: Simplified Four-Button Rating**
The system shall provide a simplified rating interface with four buttons for easier decision-making: (1) Again (quality=0) - "I don't know this at all", red button, resets card, (2) Hard (quality=2) - "I got it wrong but recognized it", orange button, short interval, (3) Good (quality=3) - "I got it right with some effort", green button, normal interval, (4) Easy (quality=5) - "I knew this perfectly", blue button, long interval. This simplified interface maps to SM-2 quality values while reducing cognitive load during reviews. Users can toggle between 4-button and 6-button modes in settings. The 4-button mode is recommended for beginners and matches popular SRS apps like Anki. Each button shows the next review date: "Again: <10min, Hard: 1d, Good: 3d, Easy: 7d" helping users make informed choices.

**FR-2.9.9: Review Statistics Tracking**
The system shall track comprehensive review statistics stored in localStorage under 'socraticdev-srs-stats': (1) Total cards in deck, (2) Cards reviewed today (resets at midnight), (3) Cards due today (calculated from nextReview), (4) Current streak (consecutive days with at least one review), (5) Longest streak (historical maximum), (6) Average ease factor (across all cards, indicates overall difficulty), (7) Review accuracy (% of reviews rated ≥3), (8) Total reviews all-time, (9) Average daily reviews (last 30 days). The SRS dashboard displays these statistics with visualizations: streak calendar heatmap (GitHub-style), review count line chart (last 30 days), accuracy pie chart, ease factor distribution histogram. Statistics provide motivation through visible progress and help users identify issues (low accuracy suggests cards are too difficult).

**FR-2.9.10: LocalStorage Persistence**
The system shall persist all flashcard data in localStorage for offline access and instant loading. Data structure: (1) 'socraticdev-srs-cards' - array of card objects with all properties (id, front, back, type, language, tags, sourceType, createdAt, interval, repetitions, easeFactor, nextReview, lastReview), (2) 'socraticdev-srs-stats' - statistics object, (3) 'socraticdev-srs-settings' - user preferences (daily limit, rating mode, auto-generation settings). The system saves to localStorage after every card creation, review, or settings change. On app load, the system reads from localStorage and initializes the SRS state. For authenticated users, data is also synced to server every 5 minutes or on app close, enabling cross-device access. LocalStorage provides instant access and offline functionality, crucial for a learning tool that should work anywhere.

**FR-2.9.11: Deck Progress Display**
The system shall categorize cards into four progress stages and display counts for each: (1) New - never reviewed (repetitions=0, nextReview in future), (2) Learning - recently started, still building memory (repetitions 1-3, interval <7 days), (3) Review - established memory, longer intervals (repetitions ≥4, interval ≥7 days), (4) Mastered - strong memory, very long intervals (repetitions ≥8, interval ≥30 days, ease_factor ≥2.5). The SRS dashboard displays these counts with a progress bar showing distribution: "New: 25 | Learning: 40 | Review: 80 | Mastered: 55". This visualization helps users understand their deck maturity and provides motivation as cards progress through stages. Users can filter the deck by stage to focus on specific categories (e.g., review only Learning cards).

### 2.10 Code Visualizer

**FR-2.10.1: Step-by-Step Execution Animation**
The system shall visualize code execution step-by-step with animated transitions between states. The visualization process: (1) User inputs code in Monaco Editor, (2) System analyzes code using AI to generate execution trace (line-by-line execution order, variable states at each step, function calls, return values), (3) System displays code with current line highlighted, (4) System shows variable panel with current values, (5) User controls execution with play/pause/step-forward/step-back buttons, (6) System animates transitions (variable value changes fade in, line highlight moves smoothly, function calls zoom to called function). The animation speed is adjustable (0.5x to 2x). This visualization helps learners build mental models of code execution, understand control flow, and debug by seeing exactly what happens at each step.

**FR-2.10.2: Call Graph Generation**
The system shall generate and display a call graph showing function relationships using ReactFlow. The call graph shows: (1) Each function as a node (labeled with function name), (2) Function calls as directed edges (caller → callee), (3) Call count as edge label (how many times function A calls function B), (4) Recursive calls as self-loops, (5) Entry point (main/top-level) highlighted in green, (6) Current execution position highlighted in yellow. The graph updates in real-time during execution animation, highlighting the active call path. Users can click nodes to jump to that function's definition or see its execution history. The call graph provides a high-level view of program structure and helps understand recursion, call stacks, and function dependencies.

**FR-2.10.3: Variable State Tracking**
The system shall track and display variable states throughout execution in a dedicated panel. The panel shows: (1) All variables in current scope (local, global), (2) Variable name, type, and current value, (3) Value history (previous values shown in gray), (4) Change indicators (green for new variables, yellow for modified values, red for deleted variables), (5) Complex data structures expanded/collapsed (arrays, objects, lists), (6) Memory addresses for reference types (showing aliasing). When execution steps forward, changed variables flash briefly to draw attention. Users can hover over variables to see full value (for long strings/arrays) and click to pin variables (keep visible even when out of scope). This panel is crucial for understanding variable mutation, scope, and data flow.

**FR-2.10.4: Multi-Language Support**
The system shall support code visualization for three languages: Python, JavaScript, and TypeScript. Each language has specific visualization features: (1) Python - shows list comprehensions step-by-step, visualizes generator state, displays decorator application, (2) JavaScript - shows closure scope chains, visualizes promise states (pending/fulfilled/rejected), displays async/await execution order, (3) TypeScript - shows type checking results, displays interface implementations, visualizes generic type resolution. The system uses language-specific parsers (Tree-sitter) and execution models to generate accurate traces. Users select language from a dropdown, and the code editor updates syntax highlighting and IntelliSense accordingly.

**FR-2.10.5: Monaco Editor Integration**
The system shall use Monaco Editor (VS Code's editor) for code input, providing professional editing experience. Monaco features enabled: (1) Syntax highlighting for all supported languages, (2) IntelliSense (autocomplete) for standard library functions, (3) Error squiggles for syntax errors, (4) Bracket matching and auto-closing, (5) Code folding for functions/classes, (6) Multiple cursors (Alt+Click), (7) Find/replace with regex support, (8) Minimap for navigation. The editor is configured with: font size 14px, tab size 4 spaces, line numbers enabled, word wrap disabled. Users can customize editor settings (font size, theme, tab size) in preferences. Monaco provides a familiar, powerful editing environment that doesn't require learning a new interface.



**FR-2.10.6: ReactFlow Graph Rendering**
The system shall use ReactFlow library to render call graphs with interactive features. ReactFlow configuration: (1) Node types - custom function nodes with name, parameter count, and execution count, (2) Edge types - directed edges with call count labels, (3) Layout - hierarchical (Dagre algorithm) with entry point at top, (4) Interactions - zoom (mouse wheel), pan (drag), node selection (click), edge highlighting (hover), (5) Minimap - shows full graph with viewport indicator, (6) Controls - zoom in/out/fit buttons. The graph updates dynamically during execution: active call path highlighted in yellow, completed calls in green, pending calls in gray. Users can export the graph as PNG/SVG for documentation. ReactFlow provides smooth 60fps performance even with 100+ nodes through canvas rendering and virtualization.

**FR-2.10.7: AI-Powered Code Analysis**
The system shall use AI (Gemini) to analyze code and generate execution traces when direct execution is not possible or practical. The AI analysis process: (1) Send code to Gemini with prompt: "Analyze this code and generate a step-by-step execution trace showing line execution order, variable states, function calls, and return values", (2) Parse AI response into structured trace format (JSON), (3) Validate trace for consistency (variables used before defined, type mismatches), (4) Generate visualization from trace. AI analysis enables visualization of: (1) Code with external dependencies (can't execute without libraries), (2) Pseudocode or algorithm descriptions, (3) Code in languages without execution environment, (4) Partial code snippets. The AI-generated trace may not be 100% accurate but provides valuable learning insights for understanding code logic.

### 2.11 Analytics Dashboard

**FR-2.11.1: Total XP Tracking**
The system shall track total experience points (XP) earned across all activities and display prominently on the analytics dashboard. XP is earned from: (1) Completing Dojo challenges (10-150 XP based on difficulty and performance), (2) Reviewing flashcards (1 XP per card), (3) Maintaining learning streaks (10 XP per day), (4) Unlocking achievements (25-500 XP based on rarity), (5) Daily quest completion (50-200 XP per quest). The dashboard displays: current total XP, XP to next league (progress bar), XP earned today, XP earned this week, XP earned this month, all-time XP rank (percentile among all users). XP provides a unified metric for progress across all platform activities and drives league progression. The system stores XP in localStorage and syncs to server for authenticated users.

**FR-2.11.2: Six-Dimensional Skill Radar**
The system shall display a skill radar chart with six dimensions representing core programming competencies: (1) Algorithms - earned from algorithm-focused challenges (Mental Compiler, Big O Battle), (2) Data Structures - earned from data structure challenges (array, tree, graph problems), (3) Debugging - earned from Code Surgery and Rubber Duck challenges, (4) Design Patterns - earned from Pattern Detective challenges, (5) Testing - earned from TDD challenges and writing test cases, (6) Architecture - earned from analyzing codebases, understanding dependencies, impact analysis. Each dimension has a 0-100 scale. The radar chart is rendered as an SVG polygon with: vertices at each dimension's value, filled area showing skill profile, gridlines at 20/40/60/80/100 for reference, dimension labels outside the polygon. Users can see their skill profile at a glance and identify areas for improvement (dimensions with low values).

**FR-2.11.3: Learning Streak Tracking**
The system shall track consecutive days with learning activity (at least one challenge completed, flashcard reviewed, or AI query made). Streak tracking: (1) Current streak - consecutive days ending today, (2) Longest streak - historical maximum, (3) Streak calendar - heatmap showing activity for last 365 days (GitHub-style), (4) Streak freeze - allows one missed day without breaking streak (earned through achievements). The dashboard displays current streak prominently with fire emoji and count: "🔥 15 day streak!". The calendar uses color intensity to show activity level: light green (1-5 activities), medium green (6-15), dark green (16+), gray (no activity). Streaks provide powerful motivation through loss aversion (don't want to break the streak) and visible progress. The system sends reminder notifications when streak is at risk (no activity by 10 PM).

**FR-2.11.4: Mode Time Tracking**
The system shall track time spent in Learning Mode vs. Building Mode to help users understand their learning patterns. Time tracking: (1) Start timer when user enters chat interface, (2) Track active time (exclude idle time >5 minutes), (3) Categorize by mode (Learning/Building), (4) Store daily totals in localStorage, (5) Display on dashboard as: total time today, time this week, time this month, mode distribution pie chart (% in each mode), average session length, most active time of day (histogram). This data helps users: (1) Ensure balanced learning (not just using Building Mode for quick answers), (2) Identify productive times (schedule learning during peak focus), (3) Track study habits (consistent daily practice vs. cramming). The system can suggest: "You've spent 80% of time in Building Mode this week. Try Learning Mode to deepen understanding!"

**FR-2.11.5: Activity Metrics**
The system shall track and display detailed activity metrics: (1) Questions asked - total AI queries submitted, (2) Code explanations received - count of AI responses with code examples, (3) Bugs caught - count of bugs found in Code Surgery challenges, (4) Challenges completed - total across all types, (5) Flashcards reviewed - total review count, (6) Projects uploaded - count of codebases analyzed, (7) Code entities explored - count of functions/classes viewed in graph visualization. The dashboard displays these metrics as: large numbers with icons, comparison to previous period ("+15% vs. last week"), sparkline charts showing trend, and achievement progress ("50/100 bugs caught for Bug Hunter achievement"). These granular metrics provide detailed insights into learning behavior and progress toward specific goals.

**FR-2.11.6: XP Progression Visualization**
The system shall display XP progression over time using an interactive line chart. The chart shows: (1) X-axis - time (last 7 days, 30 days, 90 days, or all time), (2) Y-axis - cumulative XP, (3) Line - XP growth over time with smooth curve, (4) Data points - hoverable to show exact XP and date, (5) Milestones - markers for league promotions, major achievements, (6) Trend line - linear regression showing average growth rate, (7) Projection - dotted line showing estimated future XP based on current rate. The chart helps users: (1) See progress visually (motivating), (2) Identify productive periods (steep slopes), (3) Notice plateaus (flat sections suggest need for more activity), (4) Set goals (project when they'll reach next league). Users can export chart as image for sharing progress.

**FR-2.11.7: LocalStorage Persistence**
The system shall persist all analytics data in localStorage for instant access and offline availability. Data structure: (1) 'socraticdev-analytics' - object containing: totalXP, skillRadar (6 dimensions), currentStreak, longestStreak, activityCalendar (365 days), modeTime (Learning/Building minutes), activityMetrics (questions, explanations, bugs, etc.), xpHistory (daily XP for last 90 days), (2) Data is updated after every activity (challenge completion, flashcard review, AI query), (3) Data is synced to server every 5 minutes for authenticated users, (4) On app load, data is read from localStorage and displayed immediately (no loading spinner). LocalStorage provides instant dashboard rendering and works offline, crucial for a tool users access frequently to check progress. The data size is small (<100KB even after months of use) so storage limits are not a concern.

### 2.12 Gamification System

**FR-2.12.1: Five League Tiers**
The system shall implement a five-tier league system for progression: (1) Bronze (0-499 XP) - starting league, bronze medal icon, warm brown color scheme, (2) Silver (500-1999 XP) - intermediate league, silver medal icon, cool gray color scheme, (3) Gold (2000-4999 XP) - advanced league, gold medal icon, yellow-gold color scheme, (4) Platinum (5000-9999 XP) - expert league, diamond icon, cyan color scheme, (5) Diamond (10000+ XP) - master league, crown icon, purple-violet color scheme. Each league has: unique icon, color gradient, badge design, and title (Bronze Learner, Silver Coder, Gold Developer, Platinum Engineer, Diamond Master). The current league is displayed prominently in the UI (navbar, profile, dashboard) with progress bar to next league. League promotions trigger celebration animations (confetti, sound effect, modal with new badge). Leagues provide long-term progression goals and status recognition.

**FR-2.12.2: Automatic League Promotion**
The system shall automatically promote users to higher leagues when they reach XP thresholds. Promotion process: (1) After any XP-earning activity, check if total XP crossed a threshold, (2) If yes, update current league in state and localStorage, (3) Display promotion modal with: congratulations message, new league badge (animated entrance), XP threshold reached, next league preview, share button (social media), (4) Play celebration sound effect, (5) Show confetti animation, (6) Update UI with new league colors/badge, (7) Unlock league-specific features (if any), (8) Record promotion in achievement history. Promotions are permanent (no demotion) to avoid demotivation. The system sends push notification for promotions if user is not active: "Congratulations! You've reached Silver League!" This automatic recognition provides powerful motivation and sense of achievement.



**FR-2.12.3: Daily Quests System**
The system shall provide three daily quests that reset at midnight (user's local timezone), offering focused goals and bonus XP. Quest types: (1) Challenge Quest - "Complete 3 Dojo challenges" (50 XP reward), (2) Flashcard Quest - "Review 20 flashcards" (30 XP reward), (3) Streak Quest - "Maintain your learning streak" (20 XP reward), (4) Time Quest - "Spend 30 minutes learning" (40 XP reward). Each quest shows: title, description, progress bar (e.g., "2/3 challenges"), XP reward, completion status. Completed quests show checkmark and "Claim Reward" button. Claiming triggers: XP addition, celebration animation, quest marked as claimed. Unclaimed rewards expire at midnight. The quest panel is accessible from dashboard and shows time until reset. Daily quests provide short-term goals that encourage consistent engagement and reward daily practice habits.

**FR-2.12.4: Quest Type Variety**
The system shall rotate quest types to maintain variety and encourage diverse activities. Quest rotation: (1) Each day, system randomly selects 3 quests from a pool of 12 quest types, (2) Quest types include: complete challenges (any type or specific type), review flashcards (any count), maintain streak, spend time learning, upload project, explore code graph, ask AI questions, achieve perfect scores, complete challenges in specific language, unlock achievement, (3) Quest difficulty scales with user level (Bronze: easy quests, Diamond: challenging quests), (4) No duplicate quest types on same day, (5) Quest history tracked to avoid repetitive patterns. This variety prevents monotony, exposes users to different platform features, and maintains engagement through novelty. Users can reroll one quest per day (premium feature) if they don't like the selection.

**FR-2.12.5: Sixteen Achievements**
The system shall implement 16 achievements across four rarity tiers, providing long-term goals and recognition for milestones. Achievement structure: (1) Common (4 achievements) - early milestones, 25 XP each: First Steps (complete first challenge), Getting Warmed Up (5 challenges), Memory Lane (first flashcard), Hat Trick (3-day streak), (2) Rare (4 achievements) - intermediate milestones, 50 XP each: Dedicated Learner (20 challenges), Card Collector (50 flashcards), Week Warrior (7-day streak), Rising Star (reach Silver), (3) Epic (4 achievements) - advanced milestones, 100 XP each: Century Club (100 challenges), Unstoppable (30-day streak), Golden Touch (reach Gold), Dojo Master (complete all 10 challenge types), (4) Legendary (4 achievements) - master milestones, 250 XP each: Legendary Dedication (100-day streak), Diamond Elite (reach Diamond), Memory Master (1000 flashcards), Code Sage (analyze 10 projects). Each achievement has: unique icon, rarity color, unlock condition, XP reward, unlock date, and flavor text.

**FR-2.12.6: Achievement Unlock Conditions**
The system shall check achievement unlock conditions after every relevant activity and unlock achievements when conditions are met. Condition checking: (1) After challenge completion, check challenge-count achievements, (2) After flashcard review, check flashcard-count achievements, (3) After streak update, check streak achievements, (4) After XP gain, check league achievements, (5) After project upload, check project achievements. When condition is met: (1) Mark achievement as unlocked with timestamp, (2) Add XP reward to total, (3) Display unlock notification (toast or modal), (4) Play unlock sound effect, (5) Add to achievement showcase, (6) Update achievement progress for related achievements. The system prevents duplicate unlocks by checking unlocked status before awarding. Achievement conditions are stored in code (not configurable) to ensure consistency and prevent cheating.

**FR-2.12.7: Achievement Rarity System**
The system shall use rarity tiers to create progression and excitement around achievements. Rarity visual design: (1) Common - gray background, simple icon, no glow, (2) Rare - blue background, detailed icon, subtle glow, (3) Epic - purple background, animated icon, medium glow, particle effects, (4) Legendary - gold background, animated icon, strong glow, particle effects, screen shake. Rarity affects: (1) XP reward (higher rarity = more XP), (2) Unlock difficulty (higher rarity = harder conditions), (3) Visual prominence (higher rarity = more celebration), (4) Social sharing (higher rarity = auto-suggest sharing). The rarity system creates a sense of progression from common early achievements to legendary late-game achievements, maintaining motivation through the entire learning journey. Users can filter achievements by rarity in the achievement showcase.

**FR-2.12.8: Achievement Unlock Notifications**
The system shall display prominent notifications when achievements are unlocked to celebrate the accomplishment. Notification design: (1) Modal overlay (dims background), (2) Achievement card with: rarity-colored border, animated icon (bounces in), achievement name (large text), description, XP reward ("+100 XP" with animation), unlock date, (3) Celebration effects: confetti animation (rarity-colored), sound effect (different per rarity), screen shake (legendary only), (4) Action buttons: "Share" (social media), "View All Achievements", "Continue". The modal auto-dismisses after 5 seconds or on click. For multiple achievements unlocked simultaneously (e.g., completing 100th challenge unlocks both Century Club and Dojo Master), show them sequentially with 1-second delay between. These celebrations provide powerful positive reinforcement and make achievements feel meaningful rather than just checkboxes.

**FR-2.12.9: Gamification Data Persistence**
The system shall persist all gamification data in localStorage for instant access and offline functionality. Data structure: 'socraticdev-gamification' object containing: (1) totalXP (number), (2) currentLeague (tier string), (3) weeklyXP (number, resets Sunday midnight), (4) dailyQuests (array of quest objects with progress), (5) questsResetAt (timestamp for next reset), (6) unlockedAchievements (array of achievement IDs with unlock dates), (7) achievementProgress (object mapping achievement IDs to current progress), (8) leagueHistory (array of promotion dates). Data is updated after every XP-earning activity, quest progress, or achievement unlock. For authenticated users, data syncs to server every 5 minutes and on app close, enabling cross-device progress. LocalStorage provides instant loading (no spinner) and offline functionality, crucial for maintaining engagement. The data size is small (<50KB) so storage is not a concern.

### 2.13 Landing Page and Marketing

**FR-2.13.1: Marketing Landing Page**
The system shall provide a compelling marketing landing page that communicates value proposition and drives sign-ups. Page structure: (1) Hero section - headline ("Learn to Code by Thinking, Not Copying"), subheadline (value proposition), CTA buttons ("Start Learning Free", "Watch Demo"), hero image/animation (code visualization), (2) Problem section - pain points of traditional learning (copy-paste without understanding, forgotten knowledge, lack of practice), (3) Solution section - how SocraticDev solves problems (Socratic method, spaced repetition, deliberate practice), (4) Features section - key features with icons and descriptions, (5) How It Works section - 3-step process with visuals, (6) Tech Stack section - technologies used with logos, (7) Dojo section - challenge types showcase, (8) CTA section - final push to sign up, (9) Footer - links, social media, legal. The page uses premium animations (GSAP, Framer Motion) for engagement and follows modern design trends (glassmorphism, gradients, micro-interactions).

**FR-2.13.2: Feature Highlights with Animations**
The system shall showcase key features with scroll-triggered animations that reveal content as users scroll. Animation strategy: (1) Features appear from bottom with fade-in and slide-up (stagger delay 0.1s between features), (2) Feature icons rotate 360° on appearance, (3) Feature cards have hover effects (lift up, glow, scale 1.05x), (4) Code examples type out character-by-character, (5) Screenshots slide in from sides (alternating left/right), (6) Statistics count up from 0 to target value. Animations use: (1) Intersection Observer for scroll triggers (trigger at 30% visibility), (2) GSAP for complex sequences, (3) Framer Motion for React component animations, (4) CSS transitions for hover effects. All animations respect prefers-reduced-motion (disabled or simplified for users with motion sensitivity). Animations increase engagement, guide attention, and make the page memorable.

**FR-2.13.3: Comparison Section**
The system shall provide a comparison table contrasting traditional learning approaches with SocraticDev's approach. Comparison points: (1) Traditional: "Copy-paste code" vs. SocraticDev: "Understand through questions", (2) Traditional: "Forget after exam" vs. SocraticDev: "Remember with spaced repetition", (3) Traditional: "Passive reading" vs. SocraticDev: "Active practice with 10 challenge types", (4) Traditional: "Generic examples" vs. SocraticDev: "Your actual codebase with GraphRAG", (5) Traditional: "No progress tracking" vs. SocraticDev: "Detailed analytics and gamification". The table uses: checkmarks (✓) for SocraticDev, X marks (✗) for traditional, color coding (green vs. red), icons for visual interest. The comparison is animated: rows appear one-by-one with slide-in effect, checkmarks/X marks pop in with bounce. This section addresses objections and clearly communicates differentiation.

**FR-2.13.4: Tech Stack Display**
The system shall display the technology stack with logos and descriptions to build credibility and attract technical users. Tech stack section shows: (1) Frontend technologies - React, TypeScript, Vite, Tailwind, GSAP, Monaco Editor (with logos), (2) Backend technologies - FastAPI, Python, Neo4j, Chroma, Redis, Celery (with logos), (3) AI technologies - Google Gemini, Tree-sitter (with logos). Each technology has: logo (SVG or PNG), name, brief description (one sentence), and link to official site. Logos are arranged in a grid with hover effects (scale up, glow). The section uses a subtle animation: logos fade in with stagger effect, and a connecting line draws between related technologies (e.g., React → Vite → Tailwind). This transparency builds trust with technical audiences and demonstrates the platform's modern, professional architecture.

**FR-2.13.5: How It Works Section**
The system shall explain the user journey in a clear, visual 3-step process. Steps: (1) "Upload Your Code" - drag-drop interface visual, description: "Upload your project and we'll analyze it with GraphRAG", icon: upload cloud, (2) "Ask Questions" - chat interface visual, description: "Our AI tutor guides you with Socratic questions", icon: chat bubble, (3) "Master Concepts" - progress dashboard visual, description: "Practice with challenges and track your growth", icon: trophy. Each step has: large number (1, 2, 3), icon, title, description, visual mockup, and arrow pointing to next step. The section animates: steps appear sequentially (0.5s delay between), arrows draw from left to right, mockups slide in from sides. This section reduces friction by showing exactly how the platform works, addressing the "what do I do first?" question that often prevents sign-ups.



**FR-2.13.6: Interactive Demo Section**
The system shall provide an interactive demo showcasing the Socratic method in action. Demo structure: (1) Simulated chat interface with pre-scripted conversation, (2) User question: "How do I implement binary search?", (3) AI response (Learning Mode): "Great question! Before we dive in, let's think through the concept: 1) What property must the data have for binary search to work? 2) How does binary search differ from linear search? 3) What happens to the search space after each comparison?", (4) User can click suggested answers or type their own, (5) AI provides follow-up based on answer, eventually leading to full explanation, (6) Toggle to show Building Mode response (direct implementation) for comparison. The demo is fully interactive (not just a video) so users experience the Socratic method firsthand. It auto-plays on scroll-into-view with typing animation for messages. This demo is crucial for communicating the unique value proposition—users need to experience the difference to understand it.

**FR-2.13.7: Call-to-Action Buttons**
The system shall provide prominent, compelling CTA buttons throughout the landing page to drive conversions. CTA placement: (1) Hero section - primary CTA "Start Learning Free" (large, primary color, above fold), secondary CTA "Watch Demo" (outlined, secondary color), (2) After each major section - "Get Started" or "Try It Now" buttons, (3) Final CTA section - large centered button "Join 10,000+ Developers Learning Smarter" with email input field. CTA design: (1) High contrast colors (primary color on light background), (2) Large size (min 48px height for touch targets), (3) Clear action-oriented text ("Start Learning" not "Submit"), (4) Hover effects (lift up, glow, scale 1.05x), (5) Loading state when clicked (spinner, disabled), (6) Success state (checkmark, "Redirecting..."). CTAs use psychological triggers: urgency ("Start Today"), social proof ("Join 10,000+"), value ("Free"), and clarity ("No Credit Card Required").

**FR-2.13.8: GSAP Scroll Animations**
The system shall implement smooth, professional scroll animations using GSAP (GreenSock Animation Platform) for premium feel. Animation types: (1) Parallax - background elements move slower than foreground (depth effect), (2) Reveal - elements fade in and slide up as they enter viewport, (3) Stagger - multiple elements appear sequentially with delay, (4) Pin - section stays fixed while content scrolls over it, (5) Scrub - animation progress tied to scroll position (smooth, controllable), (6) Morph - shapes transform as user scrolls. GSAP configuration: (1) ScrollTrigger plugin for scroll-based animations, (2) Trigger at 30% visibility (start: "top 70%"), (3) Once: true (don't repeat on scroll up), (4) Ease: "power2.out" for natural motion, (5) Duration: 0.8-1.2s for reveals. All animations are GPU-accelerated (transform, opacity only) for 60fps performance. Animations create a premium, polished experience that differentiates from basic landing pages.

**FR-2.13.9: Custom Cursor Effects**
The system shall implement a custom cursor on desktop (hidden on mobile/tablet) that enhances interactivity. Cursor design: (1) Small circle (32px diameter) that follows mouse with spring physics (smooth, slightly lagged), (2) Changes size/color on hover: buttons (scale 2x, primary color), links (scale 1.5x, accent color), code blocks (scale 1.2x, monospace indicator), (3) Blend mode: difference or exclusion for contrast with background, (4) Particle trail - small dots that fade out behind cursor, (5) Click ripple - expanding circle on click. Implementation: (1) Track mouse position with mousemove event, (2) Update cursor position with Framer Motion spring animation (stiffness: 700, damping: 25), (3) Detect hover targets with mouseover/mouseout events, (4) Hide default cursor with CSS (cursor: none), (5) Disable on touch devices (pointer: coarse media query). Custom cursor adds playfulness and premium feel, making the landing page memorable and engaging.

**FR-2.13.10: Scroll Progress Indicator**
The system shall display a scroll progress bar at the top of the page showing how far the user has scrolled. Progress bar design: (1) Fixed position at top of viewport (z-index: 1000), (2) Full width, 3px height, (3) Primary color gradient (left to right), (4) Smooth animation tied to scroll position (0% at top, 100% at bottom), (5) Subtle glow effect. Implementation: (1) Calculate scroll percentage: (scrollY / (documentHeight - windowHeight)) × 100, (2) Update progress bar width with Framer Motion (scaleX transform from 0 to 1), (3) Use spring physics for smooth motion (stiffness: 100, damping: 30), (4) Update on scroll event (throttled to 60fps). The progress bar provides visual feedback on page length, encourages scrolling to see more content, and adds a modern, polished touch. It's a subtle but effective engagement tool that keeps users oriented.

## 3. Non-Functional Requirements

### 3.1 Performance

**NFR-3.1.1: Landing Page Load Time**
The system shall load the landing page in less than 2 seconds on a 3G connection (1.6 Mbps download, 750 Kbps upload, 150ms RTT) to ensure accessibility for users with slower internet. Performance optimization strategies: (1) Code splitting - separate bundles for landing page vs. app pages, (2) Image optimization - WebP format with fallback, lazy loading for below-fold images, responsive images (srcset), (3) Font optimization - preload critical fonts, font-display: swap, subset fonts to include only used characters, (4) CSS optimization - critical CSS inlined, non-critical CSS loaded async, Tailwind purge removes unused styles, (5) JavaScript optimization - minification, tree shaking, defer non-critical scripts, (6) CDN delivery - static assets served from edge locations. Target metrics: First Contentful Paint <1.5s, Largest Contentful Paint <2.5s, Time to Interactive <3.5s. These targets ensure users on slower connections can access the platform, expanding reach to developing markets and rural areas.

**NFR-3.1.2: Progressive AI Response Rendering**
The system shall render AI responses progressively as they stream from the API, providing immediate feedback and reducing perceived latency. Streaming implementation: (1) Use Gemini API's streaming mode (generateContentStream), (2) Receive response chunks as they're generated (typically 10-50 tokens per chunk), (3) Append each chunk to the message display in real-time, (4) Parse markdown incrementally (update rendering as new content arrives), (5) Scroll to bottom as content grows, (6) Show typing indicator before first chunk arrives. This approach provides: (1) Faster perceived response time (user sees output immediately, not after 5-10 seconds), (2) Better user experience (can start reading while AI is still generating), (3) Ability to stop generation mid-stream if answer is sufficient. Streaming is especially important for long responses (code examples, detailed explanations) where waiting for complete response would feel slow.

**NFR-3.1.3: Redis Caching Strategy**
The system shall cache frequently accessed query results in Redis with a 5-minute TTL (Time To Live) to reduce database load and improve response times. Caching strategy: (1) Cache keys: "callers:{function_id}", "deps:{function_id}", "impact:{function_id}", "search:{query_hash}:{project_ids}", "graph:{project_id}:{filter_hash}", (2) Cache on first query (cache miss), return from cache on subsequent queries (cache hit), (3) Invalidate cache on project updates (delete all keys matching "project:{project_id}:*"), (4) Use Redis connection pooling (10 connections) for concurrency, (5) Serialize data as JSON for storage, deserialize on retrieval. Expected cache hit rate: >80% for typical usage patterns (users repeatedly query same functions/projects). Cache reduces: (1) Database query time (Redis <1ms vs. Neo4j 100-500ms), (2) Database load (fewer concurrent queries), (3) API response time (faster overall). The 5-minute TTL balances freshness (data doesn't get too stale) with efficiency (most queries hit cache).

**NFR-3.1.4: Concurrent User Support**
The system shall support 100 concurrent users with response times within specified limits for all operations. Concurrency handling: (1) FastAPI async/await for non-blocking I/O (handles 1000+ concurrent requests per worker), (2) Multiple Uvicorn workers (4-8 workers based on CPU cores), (3) Database connection pooling (Neo4j: 50 connections, Chroma: 20 connections, Redis: 10 connections), (4) Celery workers for async tasks (4-8 workers), (5) Load balancer (Nginx) for distributing requests across workers, (6) Rate limiting (100 requests/minute per user) to prevent abuse. Performance targets under 100 concurrent users: (1) API response time p95 <500ms for graph queries, (2) API response time p95 <1s for semantic search, (3) Upload processing rate >100 files/minute, (4) No request timeouts or errors. These targets ensure responsive experience even during peak usage (e.g., class of students using platform simultaneously).

**NFR-3.1.5: Monaco Editor Lazy Loading**
The system shall lazy load Monaco Editor to reduce initial bundle size and improve page load time. Lazy loading implementation: (1) Monaco Editor is ~3MB uncompressed, ~1MB gzipped - too large for initial bundle, (2) Use React.lazy() and dynamic import() to load Monaco only when needed, (3) Show loading spinner while Monaco loads (typically 1-2 seconds on fast connection), (4) Cache loaded Monaco in browser (subsequent loads are instant), (5) Preload Monaco on hover over "Code Editor" button (reduces perceived latency). Monaco is needed for: Dojo challenges (code input), Code Visualizer (code input), Project Upload (code preview). By lazy loading, initial page load is 500KB smaller, improving Time to Interactive by ~1 second. This is crucial for first-time users who may not immediately use code editing features.

**NFR-3.1.6: Route-Based Code Splitting**
The system shall split code by route to load only necessary JavaScript for each page. Code splitting strategy: (1) Landing page bundle (~200KB) - includes hero, animations, marketing content, (2) Learning Hub bundle (~150KB) - includes tool cards, navigation, (3) Chat bundle (~300KB) - includes chat interface, markdown rendering, (4) Dojo bundle (~400KB) - includes all challenge types, (5) SRS bundle (~100KB) - includes flashcard interface, SM-2 algorithm, (6) Analytics bundle (~150KB) - includes charts, statistics, (7) Shared bundle (~100KB) - common components, utilities. Implementation: (1) Use React Router with React.lazy() for route components, (2) Vite automatically creates separate chunks for lazy-loaded components, (3) Preload next likely route on hover (e.g., preload Chat when hovering "Start Learning"), (4) Show loading spinner during chunk load (typically <500ms). Code splitting reduces initial bundle from ~1.5MB to ~300KB, improving load time by 3-4 seconds on slow connections.



### 3.2 Scalability

**NFR-3.2.1: Horizontal Scaling**
The system shall scale horizontally by adding more server instances to handle increased load. Scaling architecture: (1) Stateless FastAPI workers - no session state stored in workers, enabling any worker to handle any request, (2) Load balancer (Nginx/AWS ALB) - distributes requests across workers using round-robin or least-connections algorithm, (3) Shared state in Redis - session data, cache stored centrally so all workers access same data, (4) Database read replicas - Neo4j and Chroma support read replicas for distributing query load, (5) Celery worker scaling - add more workers to process uploads faster. Scaling triggers: (1) CPU usage >70% sustained for 5 minutes, (2) Memory usage >80%, (3) Request queue depth >100, (4) Response time p95 >2s. Auto-scaling (AWS/GCP) can automatically add/remove instances based on these metrics. Horizontal scaling is preferred over vertical (bigger servers) because it's more cost-effective and provides better fault tolerance (if one instance fails, others continue serving).

**NFR-3.2.2: Async Task Processing**
The system shall use Celery distributed task queue for async processing to handle upload spikes without blocking the API. Async architecture: (1) API receives upload request, validates, creates session, queues Celery task, returns session_id immediately (response time <100ms), (2) Celery worker picks up task from RabbitMQ queue, processes files, updates session status, (3) Frontend polls status endpoint for progress updates, (4) Multiple workers process tasks concurrently (4-8 workers typical). Benefits: (1) API remains responsive during heavy processing, (2) Can handle upload spikes (queue buffers requests), (3) Failed tasks can be retried automatically, (4) Workers can be scaled independently of API, (5) Long-running tasks (10+ minutes) don't timeout. This architecture is essential for handling large project uploads (10,000 files taking 100 minutes) without degrading API performance for other users.

**NFR-3.2.3: Multi-Project Performance**
The system shall maintain consistent performance as users upload multiple projects, avoiding degradation from data growth. Performance strategies: (1) Project isolation - each project has separate Chroma collection, preventing cross-project query slowdown, (2) Indexed queries - all Neo4j queries filter by project_id (indexed), ensuring O(log n) lookup regardless of total projects, (3) Lazy loading - only load active project's data, not all projects, (4) Pagination - limit result sets to 100 items, use cursor-based pagination for more, (5) Archive old projects - move inactive projects (not accessed in 90 days) to cold storage, reducing active database size. Expected performance: (1) Query time should not increase significantly from 1 project to 100 projects (due to indexing and isolation), (2) Storage grows linearly with projects (expected), (3) Memory usage stays constant (only active project data loaded). This ensures the platform scales to power users with dozens of projects.

**NFR-3.2.4: Vector Embedding Partitioning**
The system shall partition vector embeddings by project in separate Chroma collections for efficient search and deletion. Partitioning strategy: (1) Collection naming: "project_{project_id}_embeddings", (2) Each collection contains only embeddings for one project, (3) Search queries specify which collections to search (one or multiple projects), (4) Deletion is collection-level (drop entire collection), not row-level, (5) Collections are created on-demand (first embedding for project), (6) Empty collections are deleted after 30 days. Benefits: (1) Search performance - smaller collections search faster (HNSW index scales with collection size), (2) Isolation - one project's embeddings don't affect another's search results, (3) Deletion efficiency - drop collection is instant vs. deleting millions of rows, (4) Storage optimization - can use different index parameters per collection based on size. This partitioning is crucial for maintaining sub-second search times as the platform grows to thousands of projects.

### 3.3 Reliability

**NFR-3.3.1: Database Retry Logic**
The system shall retry failed database operations up to 3 times with exponential backoff before reporting failure. Retry strategy: (1) Retry on transient errors: connection timeout, connection refused, temporary unavailability, deadlock, (2) Don't retry on permanent errors: authentication failure, query syntax error, constraint violation, (3) Backoff schedule: 1st retry after 1 second, 2nd after 2 seconds, 3rd after 4 seconds, (4) Add jitter (±20% random variation) to prevent thundering herd, (5) Log each retry attempt with error details, (6) After 3 failures, raise exception to caller with all error messages. Implementation: Use tenacity library (Python) or custom retry decorator. This retry logic handles temporary network issues, database restarts, and transient load spikes without failing user requests. Most transient errors resolve within 1-2 retries, making the system resilient to temporary infrastructure issues.

**NFR-3.3.2: API Rate Limit Handling**
The system shall handle API rate limits (Gemini API) by queuing requests and processing them when limits reset. Rate limit handling: (1) Track API usage: requests per minute, tokens per minute, (2) When rate limit hit (429 response), extract retry-after header, (3) Queue pending requests in Redis sorted set (by priority and timestamp), (4) Wait for retry-after duration, (5) Process queued requests in order, (6) Display wait time to user: "API rate limit reached. Your request will be processed in 30 seconds", (7) Allow user to cancel queued request. Rate limits: Gemini free tier typically 60 requests/minute, 32K tokens/minute. For high-volume usage, implement: (1) Request batching (combine multiple embeddings into one API call), (2) Caching (avoid redundant API calls), (3) Upgrade to paid tier (higher limits). This handling ensures the platform continues functioning during rate limit events rather than failing requests.

**NFR-3.3.3: Graceful File Processing Failures**
The system shall continue processing uploads when individual files fail, maximizing value extracted from uploads. Failure handling: (1) Wrap each file's parsing in try-except block, (2) On parse error: log error with file path, line number, error message; add to session errors list; continue to next file, (3) On embedding error: log error, skip embedding for that entity, continue processing, (4) On database error: retry 3 times, if still fails, log and continue, (5) After all files processed, display summary: "Processed 9,850 of 10,000 files. 150 files had errors. View error report." (6) Error report categorizes errors: syntax errors (120), unsupported features (20), timeouts (10), (7) Provide suggestions: "Fix syntax errors and re-upload these files." This approach ensures users get value from uploads even when some files have issues, which is common in real-world codebases with legacy code, generated files, or experimental features.

**NFR-3.3.4: Atomic Database Transactions**
The system shall use atomic transactions for operations that modify multiple database entities to maintain consistency. Transaction usage: (1) Project creation with entities and relationships - all-or-nothing (if any step fails, rollback all), (2) Project deletion - delete Neo4j nodes, Chroma embeddings, Redis cache in single transaction, (3) Project update - remove old entities, add new entities atomically, (4) User registration - create user, create default settings, create welcome notification atomically. Implementation: (1) Neo4j transactions using session.write_transaction(), (2) Chroma doesn't support transactions, so use compensating actions (delete on failure), (3) Redis transactions using MULTI/EXEC, (4) Cross-database transactions use saga pattern (sequence of local transactions with compensating actions). Atomic transactions prevent: (1) Partial data (entities without relationships), (2) Orphaned data (embeddings without Neo4j nodes), (3) Inconsistent state (cache out of sync with database). This ensures data integrity even during failures.

**NFR-3.3.5: Comprehensive Error Logging**
The system shall log all errors with full stack traces, context, and metadata for debugging and monitoring. Logging strategy: (1) Log levels: DEBUG (development only), INFO (normal operations), WARNING (recoverable issues), ERROR (handled errors), CRITICAL (unhandled errors), (2) Log format: JSON with fields: timestamp, level, logger name, message, request_id, user_id, project_id, file_path, line_number, function_name, stack_trace, metadata, (3) Log destinations: stdout (captured by container orchestrator), file (logs/app.log, rotated daily), centralized logging (future: ELK stack, CloudWatch), (4) Sensitive data filtering: mask passwords, API keys, PII before logging, (5) Correlation IDs: generate unique request_id for each API request, include in all logs for that request. Logs enable: (1) Debugging production issues, (2) Monitoring error rates and patterns, (3) Alerting on critical errors, (4) Performance analysis (slow query logs). Comprehensive logging is essential for maintaining production systems.

**NFR-3.3.6: User-Friendly Error Messages**
The system shall display user-friendly error messages that explain what went wrong and suggest next steps, without exposing internal details. Error message design: (1) User-facing message: clear, non-technical language explaining the issue, (2) Suggested actions: what user can do to resolve or work around, (3) Error code: unique identifier for support/debugging, (4) Timestamp: when error occurred, (5) Support link: how to get help if issue persists. Examples: (1) Database error → "We're having trouble connecting to our servers. Please try again in a few minutes. If the issue persists, contact support with error code DB_CONN_001.", (2) Parse error → "We couldn't parse file 'src/legacy.py' due to syntax errors on line 42. You can exclude this file and re-upload, or fix the syntax and try again.", (3) Rate limit → "We've reached our API limit for this minute. Your request is queued and will be processed in 30 seconds. You can wait or try again later." User-friendly errors reduce frustration, support burden, and abandonment while maintaining security (don't expose stack traces, database details, etc.).

---

## Document Metadata

**Document Version**: 1.0  
**Last Updated**: January 25, 2026  
**Author**: Kiro AI Assistant  
**Status**: Complete - Ready for Implementation  
**Total Requirements**: 150+ detailed functional and non-functional requirements  
**Document Length**: 2000+ lines  
**Review Status**: Pending stakeholder review  

**Change History**:
- v1.0 (2026-01-25): Initial comprehensive requirements document with extensive detail for each requirement (5-6+ lines per requirement as requested)

**Related Documents**:
- DESIGN_COMPLETE.md - Complete system design document
- PPT_GENERATION_PROMPT_FINAL.txt - Presentation generation prompt
- backend/README.md - Backend architecture overview
- frontend/README.md - Frontend architecture overview

---

**END OF REQUIREMENTS DOCUMENT**

*This comprehensive requirements document provides detailed specifications for the entire SocraticDev platform, covering all features from AI chat and GraphRAG system to The Dojo challenges, spaced repetition, analytics, and gamification. Each requirement includes extensive explanation (5-6+ lines minimum) covering purpose, implementation details, user experience, technical considerations, and business value. The document serves as the authoritative source for understanding what the system must do and forms the foundation for design, implementation, and testing.*
undle (~300KB) - includes chat interface, Monaco Editor, context panel, (4) Dojo bundle (~400KB) - includes all 10 challenge types, code editor, (5) Graph bundle (~250KB) - includes ReactFlow, graph visualization, (6) SRS bundle (~100KB) - includes flashcard interface, SM-2 algorithm, (7) Analytics bundle (~150KB) - includes charts, statistics, (8) Visualizer bundle (~200KB) - includes execution animator, call graph. Each bundle is loaded on-demand when user navigates to that route. Shared dependencies (React, Zustand, Tailwind) are in a common bundle (~150KB) loaded once. This approach: (1) Reduces initial load time (only landing page bundle needed), (2) Improves Time to Interactive (less JavaScript to parse), (3) Reduces bandwidth usage (users only download features they use), (4) Enables parallel loading (multiple bundles can load simultaneously). Vite automatically handles code splitting based on dynamic imports.

### 3.2 Scalability

**NFR-3.2.1: Database Horizontal Scaling**
The system shall support horizontal scaling of databases to handle growth in users and data. Scaling strategy: (1) Neo4j - use Neo4j Causal Cluster with 3+ core servers for read replicas, route read queries to replicas and write queries to leader, supports millions of nodes and billions of relationships, (2) Chroma - deploy multiple Chroma instances behind load balancer, partition collections by project_id (consistent hashing), supports billions of vectors across instances, (3) Redis - use Redis Cluster with sharding across 3+ nodes, automatic failover with Redis Sentinel, supports 100K+ operations/second per node. This architecture enables: (1) Adding capacity by adding servers (not upgrading existing servers), (2) High availability (no single point of failure), (3) Geographic distribution (servers in multiple regions for low latency). The application code is stateless (no session affinity required) enabling easy horizontal scaling of API servers.

**NFR-3.2.2: Celery Worker Scaling**
The system shall support dynamic scaling of Celery workers based on queue depth to handle variable upload workload. Scaling strategy: (1) Monitor Celery queue depth (number of pending tasks), (2) If queue depth >100 for >5 minutes, spawn additional workers (up to 16 workers), (3) If queue depth <10 for >10 minutes, terminate idle workers (down to 2 workers minimum), (4) Use Kubernetes HorizontalPodAutoscaler or AWS ECS Service Auto Scaling for automatic scaling, (5) Each worker processes one upload task at a time (CPU-bound parsing), (6) Workers can run on separate machines for true parallelism. This elastic scaling: (1) Handles traffic spikes (e.g., class of 50 students uploading projects simultaneously), (2) Reduces costs during low usage (fewer workers running), (3) Maintains processing rate target (100 files/minute) regardless of load. Workers are stateless and can be added/removed without affecting in-progress tasks.

**NFR-3.2.3: Multi-Project Data Isolation**
The system shall support unlimited projects per user with complete data isolation through project_id namespacing. Isolation implementation: (1) Every database record includes project_id field, (2) All queries filter by project_id (enforced at ORM/query builder level), (3) User can only access projects where user_id matches project.owner_id (enforced by API middleware), (4) Chroma collections are per-project (project_{project_id}_embeddings), (5) Redis cache keys include project_id (project:{project_id}:*), (6) File storage is per-project (uploads/{user_id}/{project_id}/). This architecture: (1) Prevents data leakage between projects (security), (2) Enables per-project deletion (delete all data for one project without affecting others), (3) Supports project sharing (add user_id to project.collaborators array), (4) Scales to millions of projects (no architectural changes needed). The isolation is enforced at multiple layers (API, database, cache) for defense in depth.

**NFR-3.2.4: Stateless API Design**
The system shall implement stateless API design where each request contains all necessary information, enabling horizontal scaling without session affinity. Stateless principles: (1) Authentication via JWT tokens (included in Authorization header), token contains user_id and expiration, no server-side session storage, (2) No in-memory state (all state in databases or client), (3) Request context passed explicitly (project_id, user_id in request body/params), (4) No sticky sessions required (any API server can handle any request), (5) Idempotent operations where possible (same request can be retried safely). Benefits: (1) Load balancer can route requests to any server (simple round-robin), (2) Servers can be added/removed without draining connections, (3) Server crashes don't lose user sessions, (4) Easier to test (no hidden state), (5) Supports serverless deployment (AWS Lambda, Cloud Functions). The only stateful component is WebSocket connections for real-time features (future), which use sticky sessions.

**NFR-3.2.5: CDN for Static Assets**
The system shall serve all static assets (JavaScript, CSS, images, fonts) through a Content Delivery Network (CDN) for global low-latency access. CDN strategy: (1) Use Cloudflare, AWS CloudFront, or similar CDN with global edge locations, (2) Configure origin server (S3, Netlify, Vercel) to serve assets, (3) Set long cache headers (1 year) for versioned assets (main.abc123.js), (4) Use cache busting through filename hashing (Vite automatically generates hashed filenames), (5) Compress assets with Brotli (better than gzip, 20% smaller), (6) Enable HTTP/2 for multiplexing (multiple files over one connection). CDN benefits: (1) Reduced latency (assets served from nearest edge location, typically <50ms), (2) Reduced origin server load (CDN handles 95%+ of requests), (3) Better availability (CDN has 99.99% uptime), (4) DDoS protection (CDN absorbs attack traffic), (5) Automatic scaling (CDN handles traffic spikes). For a global user base, CDN is essential for consistent performance.

**NFR-3.2.6: Database Connection Pooling**
The system shall use connection pooling for all database connections to reduce overhead and support high concurrency. Pooling configuration: (1) Neo4j - pool size 50, max lifetime 1 hour, connection timeout 30s, idle timeout 10 minutes, (2) Chroma - pool size 20, max lifetime 30 minutes, connection timeout 10s, (3) Redis - pool size 10, max lifetime 30 minutes, connection timeout 5s. Pool management: (1) Connections created on-demand up to pool size, (2) Idle connections kept alive with periodic health checks, (3) Failed connections removed and replaced, (4) Connections recycled after max lifetime (prevents stale connections), (5) Blocked requests wait for available connection (with timeout). Benefits: (1) Eliminates connection overhead (establishing connection takes 50-200ms), (2) Supports high concurrency (50 concurrent Neo4j queries), (3) Prevents connection exhaustion (database has max connection limit), (4) Improves reliability (automatic reconnection on failure). Without pooling, the system would be limited to ~10 concurrent users.

### 3.3 Reliability

**NFR-3.3.1: Graceful Degradation**
The system shall degrade gracefully when external services are unavailable, maintaining core functionality. Degradation strategy: (1) If Gemini API is down: show error message "AI is temporarily unavailable", allow browsing uploaded projects, graph visualization, and cached data, disable chat and AI-generated challenges, (2) If Neo4j is down: show error message "Graph database is temporarily unavailable", allow semantic search (Chroma only), disable graph visualization and relationship queries, (3) If Chroma is down: show error message "Semantic search is temporarily unavailable", allow graph queries (Neo4j only), disable semantic search and similar entity discovery, (4) If Redis is down: continue without caching (slower but functional), log warning for monitoring. The system detects failures through: (1) Connection timeouts (5-30s depending on service), (2) Health check endpoints (called every 30s), (3) Circuit breaker pattern (stop calling failed service after 3 consecutive failures, retry after 1 minute). Graceful degradation ensures users can still access their data even when some services are down.

**NFR-3.3.2: Automatic Retry with Exponential Backoff**
The system shall automatically retry failed operations with exponential backoff to handle transient failures. Retry strategy: (1) Retry on specific errors: network timeouts, 5xx server errors, rate limit errors (429), connection refused, (2) Don't retry on: 4xx client errors (except 429), authentication failures, validation errors, (3) Exponential backoff: wait 1s, 2s, 4s, 8s, 16s between retries (max 5 retries), (4) Add jitter (random 0-1s) to prevent thundering herd, (5) Log each retry attempt with error details. Retry implementation: (1) API calls - use tenacity library with retry decorator, (2) Database operations - use driver's built-in retry logic, (3) Celery tasks - configure max_retries=3, default_retry_delay=60s. Automatic retry handles: (1) Temporary network issues (packet loss, DNS failures), (2) Service restarts (brief downtime during deployment), (3) Rate limiting (wait and retry), (4) Database deadlocks (retry transaction). This significantly improves reliability without user intervention.


**NFR-3.3.3: Data Backup and Recovery**
The system shall implement automated backup and recovery procedures to prevent data loss. Backup strategy: (1) Neo4j - daily full backup at 2 AM UTC, incremental backups every 6 hours, retain 7 daily backups + 4 weekly backups + 3 monthly backups, backup to S3 with versioning enabled, (2) Chroma - daily backup of collection metadata and vectors, backup to S3, retain 7 daily backups, (3) PostgreSQL (user data) - continuous WAL archiving to S3, daily full backup, point-in-time recovery supported, (4) User uploads - stored in S3 with versioning enabled, lifecycle policy moves old versions to Glacier after 30 days. Recovery procedures: (1) Database corruption - restore from latest backup (RPO: 6 hours for Neo4j, 24 hours for Chroma), (2) Accidental deletion - restore specific project from backup, (3) Disaster recovery - restore entire system in new region from backups (RTO: 4 hours). Backups are tested monthly by restoring to staging environment. This ensures user data is never permanently lost.

**NFR-3.3.4: Health Check Endpoints**
The system shall provide health check endpoints for monitoring and load balancer health checks. Health check endpoints: (1) GET /api/health - basic health check, returns 200 OK if API is running, response time <100ms, (2) GET /api/health/detailed - detailed health check, checks all dependencies (Neo4j, Chroma, Redis, Gemini API), returns JSON with status of each service, response time <2s, (3) GET /api/health/ready - readiness check for Kubernetes, returns 200 only when all dependencies are healthy, used by load balancer to route traffic. Health check implementation: (1) Basic check - no database calls, just returns OK (fast, doesn't add load), (2) Detailed check - attempts simple query to each database (SELECT 1, ping), times out after 5s per service, (3) Ready check - same as detailed but returns 503 if any service is unhealthy. Load balancers call /health every 10s, remove unhealthy servers from rotation. Monitoring systems call /health/detailed every 60s, alert if any service is down. These endpoints enable automated failure detection and recovery.

**NFR-3.3.5: Error Logging and Monitoring**
The system shall log all errors with sufficient context for debugging and monitoring. Logging strategy: (1) Log levels - DEBUG (development only), INFO (normal operations), WARNING (recoverable errors), ERROR (unhandled exceptions), CRITICAL (system failures), (2) Log format - structured JSON with: timestamp, level, message, user_id, request_id, endpoint, error_type, stack_trace, context (relevant variables), (3) Log destinations - stdout (captured by container orchestrator), file (rotated daily, retained 7 days), external service (Sentry, DataDog, CloudWatch), (4) Sensitive data redaction - passwords, tokens, API keys redacted from logs. Monitoring: (1) Error rate alerts (>10 errors/minute triggers alert), (2) Slow query alerts (database queries >5s), (3) High memory alerts (>80% memory usage), (4) Disk space alerts (<10% free space). Logs are searchable by: user_id (find all errors for a user), request_id (trace request through system), error_type (find all instances of specific error). This enables rapid debugging and proactive issue detection.

**NFR-3.3.6: Circuit Breaker Pattern**
The system shall implement circuit breaker pattern for external service calls to prevent cascading failures. Circuit breaker states: (1) Closed (normal) - requests pass through, failures are counted, (2) Open (failing) - requests fail immediately without calling service, (3) Half-Open (testing) - limited requests pass through to test if service recovered. State transitions: (1) Closed → Open - after 5 consecutive failures or 50% failure rate over 10 requests, (2) Open → Half-Open - after 60s timeout, (3) Half-Open → Closed - after 3 consecutive successes, (4) Half-Open → Open - after any failure. Circuit breaker implementation: (1) Gemini API calls - circuit breaker with 60s timeout, (2) Neo4j queries - circuit breaker with 30s timeout, (3) Chroma queries - circuit breaker with 30s timeout. When circuit is open: (1) Return cached data if available, (2) Return error message to user, (3) Log circuit open event for monitoring. Circuit breakers prevent: (1) Wasting resources on failing calls, (2) Cascading failures (one slow service doesn't bring down entire system), (3) Overwhelming recovering services (gradual recovery through half-open state). This pattern is essential for resilient distributed systems.

### 3.4 Security

**NFR-3.4.1: Password Security**
The system shall implement industry-standard password security practices to protect user accounts. Password requirements: (1) Minimum 8 characters, (2) At least one uppercase letter, (3) At least one lowercase letter, (4) At least one number, (5) At least one special character (!@#$%^&*), (6) Not in common password list (top 10,000 breached passwords), (7) Not same as username or email. Password storage: (1) Hash with bcrypt (cost factor 12, ~250ms to hash), (2) Never store plaintext passwords, (3) Salt is automatically generated by bcrypt (unique per password), (4) Hashes stored in database with user record. Password verification: (1) Compare submitted password with stored hash using bcrypt.compare, (2) Constant-time comparison (prevents timing attacks), (3) Rate limit login attempts (5 attempts per 15 minutes per IP), (4) Lock account after 10 failed attempts (requires email verification to unlock). Password reset: (1) Generate secure random token (32 bytes), (2) Store token hash in database with expiration (24 hours), (3) Send reset link via email, (4) Verify token and allow password change, (5) Invalidate all sessions on password change. These practices protect against: brute force attacks, rainbow table attacks, credential stuffing, and password reuse attacks.

**NFR-3.4.2: JWT Authentication**
The system shall use JSON Web Tokens (JWT) for stateless authentication. JWT structure: (1) Header - algorithm (HS256), token type (JWT), (2) Payload - user_id, email, issued_at (iat), expires_at (exp), (3) Signature - HMAC-SHA256 with secret key (256-bit random key, stored in environment variable). Token lifecycle: (1) Login - generate JWT with 7-day expiration, return to client, (2) Client - store JWT in httpOnly cookie (not accessible to JavaScript, prevents XSS), (3) Requests - include JWT in Authorization header (Bearer token) or cookie, (4) Server - verify JWT signature and expiration on each request, extract user_id from payload, (5) Logout - delete cookie, add token to blacklist (Redis set with expiration). Token refresh: (1) Generate refresh token (30-day expiration) on login, (2) Client uses refresh token to get new access token when expired, (3) Refresh tokens are single-use (invalidated after use). JWT benefits: (1) Stateless (no server-side session storage), (2) Scalable (any server can verify token), (3) Secure (signed to prevent tampering), (4) Efficient (no database lookup on each request). The system uses short-lived access tokens (7 days) with refresh tokens for balance between security and user experience.

**NFR-3.4.3: Input Validation and Sanitization**
The system shall validate and sanitize all user inputs to prevent injection attacks. Validation strategy: (1) API request validation - use Pydantic models to validate request body, query params, and path params, reject requests with invalid data types or missing required fields, (2) String length limits - enforce maximum lengths (username: 50 chars, email: 100 chars, project name: 100 chars, code: 1MB), (3) Allowed characters - restrict to alphanumeric + specific special chars, reject control characters and null bytes, (4) File upload validation - check file extension whitelist (.py, .js, .ts, .java, .cpp, .cs), check MIME type, scan for malware (ClamAV), limit file size (10MB per file), (5) SQL injection prevention - use parameterized queries (never string concatenation), ORM escapes values automatically, (6) NoSQL injection prevention - validate types before database queries, don't use user input in query operators. Sanitization: (1) HTML output - escape HTML entities (<, >, &, ", ') to prevent XSS, use DOMPurify for rich text, (2) Markdown rendering - use safe markdown parser (marked with sanitize: true), strip script tags and event handlers, (3) File paths - validate paths don't contain ../ (directory traversal), normalize paths before file operations. These practices prevent: SQL injection, NoSQL injection, XSS, path traversal, and command injection attacks.

**NFR-3.4.4: Rate Limiting**
The system shall implement rate limiting to prevent abuse and ensure fair resource usage. Rate limit tiers: (1) Anonymous users - 10 requests/minute, 100 requests/hour, (2) Authenticated users - 100 requests/minute, 1000 requests/hour, (3) Premium users - 500 requests/minute, 10000 requests/hour. Rate limit implementation: (1) Use Redis for distributed rate limiting (works across multiple API servers), (2) Sliding window algorithm (more accurate than fixed window), (3) Key format: "ratelimit:{user_id}:{window}" or "ratelimit:{ip}:{window}", (4) Increment counter on each request, check if over limit, (5) Return 429 Too Many Requests if over limit with Retry-After header. Rate limit scope: (1) Per-endpoint limits - upload: 5/hour, AI query: 50/hour, graph query: 100/hour, (2) Global limit - total requests across all endpoints, (3) IP-based limit - for anonymous users, (4) User-based limit - for authenticated users. Rate limit headers: (1) X-RateLimit-Limit - max requests allowed, (2) X-RateLimit-Remaining - requests remaining in window, (3) X-RateLimit-Reset - timestamp when limit resets. Rate limiting prevents: DDoS attacks, API abuse, resource exhaustion, and ensures fair usage for all users.

**NFR-3.4.5: CORS Configuration**
The system shall configure Cross-Origin Resource Sharing (CORS) to allow only trusted origins. CORS configuration: (1) Allowed origins - production domain (socraticdev.com), staging domain (staging.socraticdev.com), localhost (development only), (2) Allowed methods - GET, POST, PUT, DELETE, OPTIONS, (3) Allowed headers - Content-Type, Authorization, X-Requested-With, (4) Exposed headers - X-RateLimit-*, X-Request-ID, (5) Credentials - allow credentials (cookies, authorization headers), (6) Max age - 86400s (24 hours, browser caches preflight response). CORS implementation: (1) Use FastAPI CORSMiddleware, (2) Validate Origin header against whitelist, (3) Return appropriate CORS headers in response, (4) Handle preflight OPTIONS requests. Security considerations: (1) Never use wildcard (*) for allowed origins in production, (2) Don't allow credentials with wildcard origins, (3) Validate Origin header (don't trust it blindly), (4) Use HTTPS for all origins (prevent MITM attacks). CORS prevents: unauthorized cross-origin requests, CSRF attacks (when combined with CSRF tokens), and data theft from malicious websites.


**NFR-3.4.6: HTTPS Enforcement**
The system shall enforce HTTPS for all connections to protect data in transit. HTTPS configuration: (1) TLS 1.2 minimum (TLS 1.3 preferred), (2) Strong cipher suites only (AES-256-GCM, ChaCha20-Poly1305), (3) Perfect Forward Secrecy (PFS) enabled, (4) HSTS header (Strict-Transport-Security: max-age=31536000; includeSubDomains; preload), (5) Redirect HTTP to HTTPS (301 permanent redirect), (6) SSL certificate from trusted CA (Let's Encrypt, DigiCert), (7) Certificate auto-renewal (certbot for Let's Encrypt). Security headers: (1) HSTS - force HTTPS for 1 year, (2) X-Content-Type-Options: nosniff - prevent MIME sniffing, (3) X-Frame-Options: DENY - prevent clickjacking, (4) X-XSS-Protection: 1; mode=block - enable XSS filter, (5) Content-Security-Policy - restrict resource loading, (6) Referrer-Policy: strict-origin-when-cross-origin - control referrer information. HTTPS benefits: (1) Encryption (prevents eavesdropping), (2) Authentication (verifies server identity), (3) Integrity (prevents tampering), (4) SEO boost (Google ranks HTTPS higher), (5) Required for modern features (service workers, geolocation). All production traffic must use HTTPS; HTTP is only allowed in local development.

**NFR-3.4.7: API Key Management**
The system shall securely manage API keys and secrets using environment variables and secret management services. Secret management: (1) Never commit secrets to version control (use .gitignore for .env files), (2) Store secrets in environment variables (12-factor app methodology), (3) Use secret management service in production (AWS Secrets Manager, HashiCorp Vault, Google Secret Manager), (4) Rotate secrets regularly (API keys every 90 days, database passwords every 180 days), (5) Use different secrets for each environment (dev, staging, production). Secret types: (1) Gemini API key - for AI completions and embeddings, (2) Database passwords - Neo4j, PostgreSQL, Redis, (3) JWT secret key - for signing tokens, (4) Encryption keys - for encrypting sensitive data, (5) OAuth client secrets - for Google/GitHub authentication. Secret access: (1) Principle of least privilege (only services that need a secret have access), (2) Audit logging (track who accessed which secrets), (3) Automatic expiration (secrets expire after set time), (4) Emergency revocation (ability to immediately revoke compromised secrets). Secret rotation: (1) Generate new secret, (2) Deploy new secret to all services, (3) Verify services using new secret, (4) Revoke old secret, (5) Update documentation. This prevents: secret leakage, unauthorized access, and limits damage from compromised secrets.

**NFR-3.4.8: SQL Injection Prevention**
The system shall prevent SQL injection attacks through parameterized queries and ORM usage. Prevention strategies: (1) Always use parameterized queries (prepared statements) - never concatenate user input into SQL strings, (2) Use ORM (SQLAlchemy) which automatically escapes values, (3) Validate input types before queries (ensure integers are integers, not strings), (4) Use whitelist for dynamic table/column names (never use user input directly), (5) Principle of least privilege (database user has only necessary permissions, not admin). Example vulnerable code: `query = f"SELECT * FROM users WHERE username = '{username}'"` (attacker can inject `' OR '1'='1`). Example safe code: `query = "SELECT * FROM users WHERE username = ?" with params=[username]` (database treats input as data, not code). Additional protections: (1) Web Application Firewall (WAF) to detect and block SQL injection attempts, (2) Database activity monitoring to detect suspicious queries, (3) Regular security audits and penetration testing, (4) Input validation (reject suspicious characters like ', --, ;). SQL injection is one of the most common and dangerous attacks; these practices are essential for security.

**NFR-3.4.9: XSS Prevention**
The system shall prevent Cross-Site Scripting (XSS) attacks through output encoding and Content Security Policy. XSS prevention strategies: (1) Output encoding - escape HTML entities (<, >, &, ", ') in all user-generated content, use React's built-in escaping (JSX automatically escapes), (2) Sanitize rich text - use DOMPurify to sanitize HTML from markdown rendering, strip script tags and event handlers, (3) Content Security Policy (CSP) - restrict script sources to same-origin and trusted CDNs, disable inline scripts (use nonce or hash), disable eval() and new Function(), (4) HttpOnly cookies - prevent JavaScript access to authentication cookies, (5) X-XSS-Protection header - enable browser's XSS filter. CSP configuration: `Content-Security-Policy: default-src 'self'; script-src 'self' 'nonce-{random}' cdn.jsdelivr.net; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' fonts.googleapis.com; connect-src 'self' api.socraticdev.com`. Types of XSS: (1) Stored XSS - malicious script stored in database, executed when viewed, (2) Reflected XSS - malicious script in URL parameter, executed immediately, (3) DOM-based XSS - malicious script manipulates DOM, executed client-side. These practices prevent all three types and protect users from account hijacking, data theft, and malware.

**NFR-3.4.10: CSRF Protection**
The system shall prevent Cross-Site Request Forgery (CSRF) attacks through CSRF tokens and SameSite cookies. CSRF prevention strategies: (1) CSRF tokens - generate random token on page load, include in forms and AJAX requests, verify token on server before processing request, (2) SameSite cookies - set SameSite=Lax or Strict on authentication cookies (prevents cookies from being sent in cross-site requests), (3) Origin/Referer validation - check Origin or Referer header matches expected domain, (4) Custom headers - require X-Requested-With header for AJAX requests (browsers don't send custom headers in cross-site requests). CSRF token implementation: (1) Generate token using cryptographically secure random (32 bytes), (2) Store token in session or JWT, (3) Include token in hidden form field or X-CSRF-Token header, (4) Verify token matches on server, (5) Reject requests with missing or invalid tokens. SameSite cookie configuration: `Set-Cookie: session=abc123; SameSite=Lax; Secure; HttpOnly`. CSRF attacks: attacker tricks user into submitting malicious request to victim site while authenticated (e.g., transfer money, change password). These protections ensure requests originate from legitimate site, not attacker's site.

### 3.5 Usability

**NFR-3.5.1: Responsive Design**
The system shall provide responsive design that adapts to all screen sizes from mobile (320px) to desktop (2560px+). Responsive breakpoints: (1) Mobile - 320px to 640px (sm), single column layout, hamburger menu, touch-optimized buttons (min 44px), (2) Tablet - 641px to 1024px (md), two column layout, collapsible sidebar, larger touch targets, (3) Desktop - 1025px to 1920px (lg/xl), three column layout, persistent sidebar, hover interactions, (4) Large desktop - 1921px+ (2xl), max-width container (1920px), centered content, larger fonts. Responsive techniques: (1) Fluid typography - font sizes scale with viewport (clamp(14px, 2vw, 18px)), (2) Flexible images - max-width: 100%, height: auto, (3) CSS Grid/Flexbox - layouts adapt to available space, (4) Media queries - different styles for different breakpoints, (5) Mobile-first approach - design for mobile, enhance for desktop. Testing: (1) Test on real devices (iPhone, Android, iPad), (2) Test in browser dev tools (responsive mode), (3) Test landscape and portrait orientations, (4) Test with different zoom levels (125%, 150%). Responsive design ensures: accessibility for all users, better mobile SEO, consistent experience across devices.

**NFR-3.5.2: Keyboard Navigation**
The system shall support full keyboard navigation for accessibility and power users. Keyboard support: (1) Tab navigation - all interactive elements (buttons, links, inputs) are focusable in logical order, (2) Focus indicators - visible outline on focused elements (2px solid primary color), (3) Skip links - "Skip to main content" link at top (hidden until focused), (4) Keyboard shortcuts - Ctrl+K (search), Ctrl+/ (help), Esc (close modal), Enter (submit), Space (toggle), (5) Arrow keys - navigate lists, menus, and tabs, (6) Home/End - jump to start/end of lists, (7) Ctrl+Enter - submit forms (alternative to clicking button). Focus management: (1) Focus trap in modals (Tab cycles within modal, Esc closes), (2) Focus restoration (return focus to trigger element when modal closes), (3) Focus on first input when form opens, (4) Skip disabled elements in tab order. ARIA attributes: (1) aria-label for icon buttons, (2) aria-expanded for collapsible sections, (3) aria-selected for tabs, (4) aria-live for dynamic content updates. Keyboard navigation is essential for: blind users (screen readers), motor-impaired users (can't use mouse), power users (keyboard is faster than mouse).

**NFR-3.5.3: Loading States**
The system shall provide clear loading states for all asynchronous operations to manage user expectations. Loading state types: (1) Skeleton screens - gray placeholder boxes matching content layout, shown while data loads, (2) Spinners - rotating circle for short operations (<3s), (3) Progress bars - percentage-based for long operations (upload, processing), (4) Optimistic updates - show expected result immediately, revert if operation fails, (5) Streaming content - show partial results as they arrive (AI responses). Loading state design: (1) Consistent placement (center of container or inline with content), (2) Appropriate size (match content size), (3) Subtle animation (smooth, not distracting), (4) Timeout handling (show error if loading >30s), (5) Cancellation option (allow user to cancel long operations). Loading messages: (1) Generic - "Loading...", (2) Specific - "Analyzing code...", "Generating embeddings...", "Searching graph...", (3) Progress - "Processing file 50/100", "75% complete". Loading states prevent: user confusion (is it working?), repeated clicks (impatience), perceived slowness (skeleton screens make app feel faster). Well-designed loading states significantly improve perceived performance.

**NFR-3.5.4: Error Messages**
The system shall provide clear, actionable error messages that help users understand and resolve issues. Error message principles: (1) Clear - explain what went wrong in plain language, (2) Specific - identify the exact problem (not generic "Error occurred"), (3) Actionable - suggest how to fix the problem, (4) Polite - don't blame the user, (5) Contextual - show error near relevant UI element. Error message structure: (1) Title - brief summary (e.g., "Upload Failed"), (2) Description - detailed explanation (e.g., "The file 'app.py' contains syntax errors on line 42"), (3) Action - what to do next (e.g., "Fix the syntax error and try again"), (4) Support - link to help docs or contact support. Error types: (1) Validation errors - inline with form fields, red border and text, (2) Network errors - toast notification, retry button, (3) Server errors - modal with error details, report button, (4) Not found errors - custom 404 page with navigation. Error codes: (1) User-friendly code (ERR_UPLOAD_001), (2) Include in error message for support, (3) Log detailed error server-side. Examples: Good - "Password must be at least 8 characters. Yours is 6 characters." Bad - "Invalid input." Good error messages reduce support requests and improve user satisfaction.


**NFR-3.5.5: Consistent UI Patterns**
The system shall maintain consistent UI patterns across all features to reduce cognitive load and improve learnability. Consistency areas: (1) Layout - consistent header, sidebar, main content structure across all pages, (2) Navigation - consistent menu structure, breadcrumbs, back buttons, (3) Colors - consistent color scheme (primary, secondary, success, warning, error, neutral), (4) Typography - consistent font families (headings, body, code), sizes, and weights, (5) Spacing - consistent padding and margins (4px, 8px, 16px, 24px, 32px scale), (6) Components - reusable Button, Card, Input, Modal components with consistent API, (7) Icons - consistent icon set (Heroicons, Lucide), size (16px, 20px, 24px), and style (outline vs. solid), (8) Interactions - consistent hover effects, click feedback, transitions. Design system: (1) Document all patterns in Storybook or Figma, (2) Create reusable React components, (3) Use Tailwind for consistent styling, (4) Enforce patterns through code review. Benefits: (1) Faster learning (users learn patterns once, apply everywhere), (2) Reduced errors (predictable behavior), (3) Faster development (reuse components), (4) Professional appearance (cohesive design). Consistency is one of the most important usability principles.

**NFR-3.5.6: Undo/Redo Functionality**
The system shall provide undo/redo functionality for destructive actions to prevent accidental data loss. Undo support: (1) Code editor - Ctrl+Z/Ctrl+Y for undo/redo (Monaco Editor built-in), (2) Flashcard deletion - "Undo" button in toast notification (5s timeout), (3) Project deletion - soft delete with 30-day recovery period, (4) Challenge reset - "Undo Reset" button (1 minute timeout), (5) Settings changes - "Revert to Defaults" button. Undo implementation: (1) Store previous state before action, (2) Show undo notification immediately after action, (3) Restore previous state if undo clicked, (4) Clear undo history after timeout or page navigation. Confirmation dialogs: (1) Show for irreversible actions (permanent delete, account closure), (2) Require typing confirmation text for critical actions ("Type DELETE to confirm"), (3) Explain consequences clearly ("This will permanently delete 500 flashcards"). Undo functionality: (1) Reduces anxiety (users feel safe to experiment), (2) Prevents data loss (easy to recover from mistakes), (3) Improves efficiency (faster than re-doing work). This is especially important for learning platforms where users are experimenting and making mistakes.

### 3.6 Accessibility

**NFR-3.6.1: WCAG 2.1 AA Compliance**
The system shall comply with Web Content Accessibility Guidelines (WCAG) 2.1 Level AA to ensure accessibility for users with disabilities. WCAG principles: (1) Perceivable - information must be presentable to users in ways they can perceive, (2) Operable - UI components must be operable by all users, (3) Understandable - information and operation must be understandable, (4) Robust - content must be robust enough to work with assistive technologies. Key requirements: (1) Color contrast - minimum 4.5:1 for normal text, 3:1 for large text (18px+), (2) Keyboard navigation - all functionality available via keyboard, (3) Focus indicators - visible focus outline on all interactive elements, (4) Alt text - descriptive alt text for all images, (5) Form labels - all inputs have associated labels, (6) Heading hierarchy - proper h1-h6 structure, (7) ARIA attributes - proper use of roles, states, and properties, (8) Captions - captions for video content, (9) Resize text - content readable at 200% zoom, (10) No flashing content - avoid content that flashes >3 times per second. Testing: (1) Automated testing (axe, Lighthouse), (2) Manual testing with screen readers (NVDA, JAWS, VoiceOver), (3) Keyboard-only testing, (4) Color blindness simulation. Compliance ensures: legal compliance (ADA, Section 508), broader user base, better SEO, improved usability for all users.

**NFR-3.6.2: Screen Reader Support**
The system shall provide full screen reader support for blind and visually impaired users. Screen reader optimization: (1) Semantic HTML - use proper elements (button, nav, main, article) instead of divs, (2) ARIA landmarks - role="navigation", role="main", role="complementary" for page regions, (3) ARIA labels - aria-label for icon buttons and complex widgets, (4) ARIA live regions - aria-live="polite" for dynamic content updates (AI responses, notifications), (5) ARIA descriptions - aria-describedby for additional context, (6) Skip links - "Skip to main content" link at top of page, (7) Focus management - move focus to relevant content after actions (e.g., focus on error message after form submission). Screen reader testing: (1) NVDA (Windows, free), (2) JAWS (Windows, commercial), (3) VoiceOver (macOS/iOS, built-in), (4) TalkBack (Android, built-in). Common issues: (1) Images without alt text, (2) Buttons without labels, (3) Form inputs without labels, (4) Dynamic content without announcements, (5) Poor heading structure. Screen reader support is essential for: blind users (primary interface), low vision users (supplement visual interface), cognitive disabilities (auditory reinforcement). Approximately 2% of users rely on screen readers.

**NFR-3.6.3: Color Contrast Requirements**
The system shall meet WCAG AA color contrast requirements for all text and interactive elements. Contrast requirements: (1) Normal text (<18px or <14px bold) - minimum 4.5:1 contrast ratio, (2) Large text (≥18px or ≥14px bold) - minimum 3:1 contrast ratio, (3) UI components (buttons, inputs, icons) - minimum 3:1 contrast ratio, (4) Focus indicators - minimum 3:1 contrast ratio. Color palette: (1) Primary color (#3B82F6 blue) on white background - 4.6:1 contrast (passes AA), (2) Success color (#10B981 green) on white background - 3.4:1 contrast (passes AA for large text), (3) Error color (#EF4444 red) on white background - 4.5:1 contrast (passes AA), (4) Text color (#1F2937 dark gray) on white background - 16.1:1 contrast (passes AAA). Dark mode: (1) Text color (#F9FAFB light gray) on dark background (#111827) - 15.8:1 contrast (passes AAA), (2) All colors adjusted for dark mode to maintain contrast ratios. Testing: (1) Use contrast checker tools (WebAIM, Stark), (2) Test with color blindness simulators, (3) Test in bright sunlight (mobile), (4) Test with low vision (zoom to 200%). Sufficient contrast ensures: readability for low vision users, usability in bright environments, compliance with accessibility standards.

**NFR-3.6.4: Alternative Text for Images**
The system shall provide descriptive alternative text for all images to support screen reader users. Alt text guidelines: (1) Describe the content and function of the image, (2) Be concise (typically <150 characters), (3) Don't start with "Image of" or "Picture of" (screen readers announce it's an image), (4) For decorative images, use empty alt="" (screen readers skip), (5) For complex images (charts, diagrams), provide long description in adjacent text or aria-describedby. Alt text examples: (1) Logo - "SocraticDev logo", (2) Icon button - "Delete flashcard", (3) Screenshot - "Code editor showing Python function with syntax highlighting", (4) Chart - "Bar chart showing XP growth over last 30 days, ranging from 50 XP to 200 XP", (5) Decorative - alt="" (empty). Dynamic images: (1) User avatars - alt="[Username]'s profile picture", (2) Code screenshots - alt="Code snippet: [first line of code]", (3) Graph visualizations - alt="Dependency graph with [N] nodes and [M] edges". Implementation: (1) Require alt attribute on all img elements (ESLint rule), (2) Validate alt text is not empty for non-decorative images, (3) Review alt text in code review. Alt text is essential for: blind users (only way to understand images), users with images disabled (slow connections), SEO (search engines use alt text).

**NFR-3.6.5: Keyboard Shortcuts Documentation**
The system shall document all keyboard shortcuts in an accessible help dialog. Keyboard shortcuts: (1) Global - Ctrl+K (search), Ctrl+/ (help), Esc (close modal), (2) Navigation - Tab (next element), Shift+Tab (previous element), Arrow keys (navigate lists), (3) Editor - Ctrl+S (save), Ctrl+Z (undo), Ctrl+Y (redo), Ctrl+F (find), (4) Chat - Ctrl+Enter (send message), Up arrow (previous message), (5) Dojo - Space (next challenge), R (reset), H (hint). Help dialog: (1) Accessible via Ctrl+/ or "?" button in header, (2) Organized by category (Global, Navigation, Editor, etc.), (3) Shows shortcut key combination and description, (4) Searchable (filter shortcuts by name), (5) Keyboard navigable (Tab through shortcuts, Esc to close). Shortcut design: (1) Use standard conventions (Ctrl+S for save, Ctrl+Z for undo), (2) Avoid conflicts with browser shortcuts, (3) Provide alternatives for Mac (Cmd instead of Ctrl), (4) Don't require complex combinations (max 2 modifiers), (5) Make shortcuts discoverable (show in tooltips, menus). Keyboard shortcuts: (1) Improve efficiency for power users, (2) Essential for keyboard-only users, (3) Reduce mouse usage (better for RSI), (4) Professional feel (matches desktop apps).

### 3.7 Maintainability

**NFR-3.7.1: Code Documentation**
The system shall maintain comprehensive code documentation for all modules, functions, and classes. Documentation requirements: (1) Module docstrings - describe module purpose, main classes/functions, usage examples, (2) Function docstrings - describe purpose, parameters (type, description), return value (type, description), exceptions raised, examples, (3) Class docstrings - describe class purpose, attributes, methods, usage examples, (4) Inline comments - explain complex logic, non-obvious decisions, workarounds, (5) Type hints - all function parameters and return values have type annotations (Python 3.10+ syntax). Documentation format: (1) Python - Google style docstrings, (2) TypeScript - JSDoc comments, (3) Markdown - README files for each major module. Documentation tools: (1) Sphinx for Python API docs, (2) TypeDoc for TypeScript API docs, (3) Storybook for React component docs. Documentation standards: (1) Write docs when writing code (not after), (2) Update docs when changing code, (3) Review docs in code review, (4) Generate docs automatically in CI/CD. Benefits: (1) Easier onboarding (new developers understand code faster), (2) Reduced bugs (clear contracts prevent misuse), (3) Better maintenance (understand why code exists), (4) Self-service (developers find answers without asking).


**NFR-3.7.2: Code Style Consistency**
The system shall enforce consistent code style across the codebase through automated tools. Style enforcement: (1) Python - Black formatter (line length 100, no configuration), Flake8 linter (PEP 8 compliance), isort for import sorting, mypy for type checking, (2) TypeScript - Prettier formatter (single quotes, 2 space indent, trailing commas), ESLint (Airbnb config), (3) Git hooks - pre-commit hook runs formatters and linters, prevents commits with style violations. Style configuration: (1) .prettierrc for Prettier config, (2) .eslintrc for ESLint config, (3) pyproject.toml for Black/Flake8 config, (4) .editorconfig for editor settings (indent size, line endings). CI/CD integration: (1) Run linters in CI pipeline, (2) Fail build if style violations found, (3) Generate style report, (4) Auto-fix violations where possible. Benefits: (1) Consistent code (easier to read and review), (2) Reduced bikeshedding (no debates about style), (3) Catch bugs (linters find potential issues), (4) Faster reviews (focus on logic, not style). Consistent style is essential for team collaboration and long-term maintainability.

**NFR-3.7.3: Modular Architecture**
The system shall use modular architecture with clear separation of concerns and minimal coupling. Architecture principles: (1) Separation of concerns - each module has single responsibility, (2) Loose coupling - modules depend on interfaces, not implementations, (3) High cohesion - related functionality grouped together, (4) Dependency injection - dependencies passed as parameters, not hardcoded, (5) Interface segregation - small, focused interfaces. Module structure: (1) Frontend - features (chat, dojo, graph), components (reusable UI), services (API clients), store (state management), utils (helpers), (2) Backend - api (routes), services (business logic), models (data structures), utils (helpers), config (settings). Module boundaries: (1) Features don't import from other features (use shared components), (2) Services don't import from API (API calls services), (3) Models don't import from services (models are pure data), (4) Utils don't import from features (utils are generic). Benefits: (1) Easier testing (mock dependencies), (2) Easier refactoring (change one module without affecting others), (3) Parallel development (teams work on different modules), (4) Code reuse (modules can be extracted to libraries). Modular architecture is essential for scaling development teams and maintaining large codebases.

**NFR-3.7.4: Automated Testing**
The system shall maintain automated test suite with high coverage for all critical functionality. Test types: (1) Unit tests - test individual functions/classes in isolation, mock dependencies, fast (<1s per test), (2) Integration tests - test multiple components together, use test databases, slower (1-10s per test), (3) End-to-end tests - test full user flows, use real browser, slowest (10-60s per test). Test coverage targets: (1) Unit tests - 80% code coverage for services and utils, (2) Integration tests - all API endpoints, all database operations, (3) E2E tests - critical user flows (signup, upload, chat, challenges). Test frameworks: (1) Python - pytest for unit/integration tests, pytest-cov for coverage, (2) TypeScript - Vitest for unit tests, React Testing Library for component tests, Playwright for E2E tests. Test organization: (1) Mirror source structure (tests/unit/services/test_query_service.py), (2) One test file per source file, (3) Descriptive test names (test_query_service_returns_top_20_results), (4) Arrange-Act-Assert pattern. CI/CD integration: (1) Run tests on every commit, (2) Fail build if tests fail or coverage drops, (3) Generate coverage report, (4) Run E2E tests nightly (too slow for every commit). Benefits: (1) Catch bugs early (before production), (2) Enable refactoring (tests verify behavior unchanged), (3) Documentation (tests show how to use code), (4) Confidence (deploy without fear).

**NFR-3.7.5: Version Control Best Practices**
The system shall follow Git best practices for version control and collaboration. Git workflow: (1) Main branch - production-ready code, protected (no direct commits), (2) Develop branch - integration branch for features, (3) Feature branches - one branch per feature (feature/add-dojo-challenges), (4) Hotfix branches - urgent production fixes (hotfix/fix-login-bug). Commit practices: (1) Atomic commits - one logical change per commit, (2) Descriptive messages - "Add Parsons Problem challenge type" not "Update files", (3) Conventional commits - prefix with type (feat:, fix:, docs:, refactor:, test:), (4) Reference issues - include issue number (#123) in commit message. Branch practices: (1) Short-lived branches - merge within 1-2 days, (2) Rebase before merge - keep history linear, (3) Delete after merge - keep branch list clean, (4) Pull before push - avoid conflicts. Code review: (1) All changes reviewed before merge, (2) At least one approval required, (3) CI checks must pass, (4) Review for: correctness, style, tests, documentation, security. Benefits: (1) Clear history (understand why changes made), (2) Easy rollback (revert specific changes), (3) Parallel development (multiple features simultaneously), (4) Quality control (code review catches issues).

**NFR-3.7.6: Dependency Management**
The system shall manage dependencies explicitly and keep them up-to-date. Dependency management: (1) Python - requirements.txt for production dependencies, requirements-dev.txt for development dependencies, pin exact versions (fastapi==0.104.1), (2) TypeScript - package.json for dependencies, package-lock.json for exact versions, use semantic versioning (^18.0.0 allows 18.x.x), (3) Docker - pin base image versions (python:3.11.5-slim), multi-stage builds to minimize image size. Dependency updates: (1) Automated updates - Dependabot creates PRs for dependency updates weekly, (2) Security updates - apply immediately (within 24 hours), (3) Major updates - test thoroughly before applying, (4) Deprecated dependencies - replace before end-of-life. Dependency auditing: (1) Run npm audit / pip-audit weekly, (2) Fix high/critical vulnerabilities immediately, (3) Document known vulnerabilities with mitigation plans, (4) Use Snyk or similar for continuous monitoring. Benefits: (1) Security (patch vulnerabilities quickly), (2) Stability (pinned versions prevent unexpected breakage), (3) Reproducibility (same versions in all environments), (4) Compliance (track licenses). Dependency management is critical for security and stability.

### 3.8 Testability

**NFR-3.8.1: Unit Test Coverage**
The system shall maintain minimum 80% unit test coverage for all service and utility modules. Coverage measurement: (1) Use pytest-cov for Python, (2) Use Vitest coverage for TypeScript, (3) Measure line coverage (% of lines executed), (4) Measure branch coverage (% of branches taken), (5) Generate HTML coverage report. Coverage targets: (1) Services - 85% coverage (core business logic), (2) Utils - 90% coverage (pure functions, easy to test), (3) API routes - 70% coverage (integration tests more important), (4) Models - 60% coverage (mostly data structures). Excluded from coverage: (1) Test files themselves, (2) Configuration files, (3) Type definitions, (4) Generated code. Coverage enforcement: (1) CI fails if coverage drops below threshold, (2) Coverage report in PR comments, (3) Identify untested code in code review. Testing best practices: (1) Test behavior, not implementation, (2) One assertion per test (or related assertions), (3) Use descriptive test names, (4) Mock external dependencies, (5) Test edge cases and error conditions. High coverage ensures: (1) Confidence in refactoring, (2) Fewer production bugs, (3) Better code design (testable code is well-designed).

**NFR-3.8.2: Integration Test Suite**
The system shall maintain comprehensive integration test suite covering all API endpoints and database operations. Integration test scope: (1) API endpoints - test all routes with valid/invalid inputs, authentication, authorization, error handling, (2) Database operations - test CRUD operations, transactions, constraints, indexes, (3) External services - test API clients with mocked responses, (4) Background tasks - test Celery tasks with test queue. Test environment: (1) Separate test database (Neo4j, PostgreSQL, Redis), (2) Reset database before each test (clean slate), (3) Use test fixtures for common data, (4) Use test API keys (not production keys). Test organization: (1) tests/integration/ directory, (2) One file per API module (test_upload_api.py), (3) Use pytest fixtures for setup/teardown, (4) Use pytest markers for test categories (@pytest.mark.slow). Test execution: (1) Run locally before committing, (2) Run in CI on every commit, (3) Run against staging environment before deployment. Integration tests catch: (1) API contract violations, (2) Database schema issues, (3) Service integration problems, (4) Configuration errors. These tests are slower than unit tests but catch different types of bugs.

**NFR-3.8.3: End-to-End Test Coverage**
The system shall maintain E2E tests for all critical user flows to ensure the system works as a whole. Critical flows: (1) User registration and login, (2) Project upload and processing, (3) AI chat conversation, (4) Dojo challenge completion, (5) Flashcard review session, (6) Graph visualization and exploration. E2E test framework: (1) Playwright for browser automation, (2) Test in Chrome, Firefox, Safari, (3) Test on desktop and mobile viewports, (4) Record videos of test runs for debugging. Test structure: (1) Page Object Model - separate page logic from test logic, (2) Reusable actions - login(), uploadProject(), askQuestion(), (3) Assertions - verify UI state, API responses, database state, (4) Screenshots - capture on failure for debugging. Test execution: (1) Run nightly (too slow for every commit), (2) Run before releases, (3) Run on staging environment (not production), (4) Parallel execution (4-8 workers) for speed. E2E tests catch: (1) UI bugs (broken layouts, missing elements), (2) Integration issues (frontend-backend mismatch), (3) Browser compatibility issues, (4) User experience problems. These tests provide highest confidence but are slowest and most brittle.


**NFR-3.8.4: Test Data Management**
The system shall provide test data fixtures and factories for consistent, maintainable test data. Test data strategies: (1) Fixtures - predefined test data loaded before tests (pytest fixtures, JSON files), (2) Factories - generate test data programmatically (Factory Boy for Python, Faker for random data), (3) Builders - fluent API for constructing test objects. Test data requirements: (1) Realistic - resembles production data, (2) Minimal - only necessary data for test, (3) Isolated - each test has independent data, (4) Deterministic - same data every run (no random values in critical tests). Example fixtures: (1) test_user - standard user account, (2) test_project - small project with 10 files, (3) test_function - sample function entity, (4) test_embedding - sample 768-dim vector. Factory example: `UserFactory.create(username="testuser", email="test@example.com")` generates user with defaults for other fields. Benefits: (1) Faster test writing (reuse fixtures), (2) Consistent tests (same data every run), (3) Easier maintenance (update fixture once, affects all tests), (4) Better readability (descriptive fixture names). Test data management is essential for maintainable test suites.

**NFR-3.8.5: Mocking External Services**
The system shall mock all external service calls in unit and integration tests to ensure fast, reliable, isolated tests. Services to mock: (1) Gemini API - mock completions and embeddings, (2) Neo4j - use in-memory database or mock driver, (3) Chroma - use in-memory collection or mock client, (4) Redis - use fakeredis library, (5) Email service - mock SMTP client. Mocking strategies: (1) Dependency injection - pass mock as parameter, (2) Monkey patching - replace module attribute with mock (pytest monkeypatch), (3) Mock libraries - unittest.mock for Python, vitest.mock for TypeScript, (4) Test doubles - fake implementations for testing. Mock configuration: (1) Return realistic responses, (2) Simulate errors (timeouts, rate limits), (3) Verify calls (assert mock called with correct arguments), (4) Reset mocks between tests. Benefits: (1) Fast tests (no network calls), (2) Reliable tests (no external dependencies), (3) Isolated tests (test one component at a time), (4) Test error handling (simulate failures). Mocking is essential for unit testing; integration tests use real services in test environment.

### 3.9 Deployment

**NFR-3.9.1: Docker Containerization**
The system shall use Docker containers for consistent deployment across environments. Container strategy: (1) Frontend - Nginx container serving static files, (2) Backend - Python container running FastAPI with Uvicorn, (3) Celery - Python container running Celery workers, (4) Neo4j - official Neo4j container, (5) Chroma - official Chroma container, (6) Redis - official Redis container. Dockerfile best practices: (1) Multi-stage builds - separate build and runtime stages, (2) Minimal base images - use alpine or slim variants, (3) Layer caching - order commands from least to most frequently changed, (4) Non-root user - run as non-root for security, (5) Health checks - HEALTHCHECK instruction for container health. Docker Compose: (1) docker-compose.yml for development (all services), (2) docker-compose.prod.yml for production (external databases), (3) Environment variables for configuration, (4) Named volumes for data persistence. Benefits: (1) Consistency (same environment everywhere), (2) Isolation (dependencies contained), (3) Portability (run anywhere Docker runs), (4) Scalability (easy to replicate containers). Docker is the foundation for modern deployment.

**NFR-3.9.2: CI/CD Pipeline**
The system shall implement automated CI/CD pipeline for continuous integration and deployment. CI pipeline (GitHub Actions, GitLab CI, or Jenkins): (1) Trigger on every commit to main/develop, (2) Checkout code, (3) Install dependencies, (4) Run linters (Black, Flake8, ESLint, Prettier), (5) Run unit tests with coverage, (6) Run integration tests, (7) Build Docker images, (8) Push images to registry (Docker Hub, ECR, GCR), (9) Deploy to staging environment, (10) Run E2E tests against staging, (11) Notify team (Slack, email). CD pipeline: (1) Trigger on tag push (v1.2.3), (2) Run full CI pipeline, (3) Deploy to production (blue-green or canary), (4) Run smoke tests, (5) Monitor for errors, (6) Rollback if errors detected. Pipeline configuration: (1) .github/workflows/ci.yml for GitHub Actions, (2) Separate jobs for parallel execution, (3) Cache dependencies for speed, (4) Fail fast on errors. Benefits: (1) Fast feedback (know if code works within minutes), (2) Consistent builds (same process every time), (3) Reduced manual work (no manual deployments), (4) Higher quality (automated testing catches bugs).

**NFR-3.9.3: Environment Configuration**
The system shall support multiple deployment environments with environment-specific configuration. Environments: (1) Development - local machine, hot reload, debug logging, test data, (2) Staging - cloud deployment, production-like, test data, (3) Production - cloud deployment, real data, monitoring, backups. Configuration management: (1) Environment variables for secrets and environment-specific values, (2) .env files for local development (not committed), (3) Secret management service for production (AWS Secrets Manager, Vault), (4) Config files for static configuration (settings.py, config.ts). Configuration structure: (1) DATABASE_URL - database connection string, (2) GEMINI_API_KEY - AI API key, (3) JWT_SECRET - token signing key, (4) ENVIRONMENT - dev/staging/prod, (5) LOG_LEVEL - debug/info/warning/error. Configuration validation: (1) Validate required variables on startup, (2) Fail fast if configuration invalid, (3) Log configuration (redact secrets), (4) Document all configuration options. Benefits: (1) Security (secrets not in code), (2) Flexibility (same code, different config), (3) Testability (use test config in tests), (4) Compliance (audit configuration changes).

**NFR-3.9.4: Zero-Downtime Deployment**
The system shall support zero-downtime deployments to avoid service interruptions. Deployment strategies: (1) Blue-green deployment - run two identical environments (blue=current, green=new), switch traffic to green after validation, keep blue for rollback, (2) Rolling deployment - gradually replace old instances with new instances, (3) Canary deployment - deploy to small subset of users first, monitor for errors, gradually increase traffic. Implementation: (1) Load balancer routes traffic to healthy instances, (2) Health checks determine instance health, (3) Graceful shutdown (finish in-flight requests before stopping), (4) Database migrations run before deployment (backward compatible), (5) Feature flags for risky changes (enable gradually). Rollback procedure: (1) Detect issues (error rate spike, latency increase), (2) Switch traffic back to old version, (3) Investigate issues, (4) Fix and redeploy. Zero-downtime requirements: (1) Stateless application (no session affinity), (2) Backward compatible changes (old and new versions coexist), (3) Database migrations are additive (don't break old version). Benefits: (1) No user impact (service always available), (2) Faster deployments (no maintenance windows), (3) Easy rollback (switch traffic back), (4) Higher deployment frequency (deploy multiple times per day).

**NFR-3.9.5: Monitoring and Alerting**
The system shall implement comprehensive monitoring and alerting for production health. Monitoring metrics: (1) Application metrics - request rate, error rate, response time (p50, p95, p99), active users, (2) Infrastructure metrics - CPU usage, memory usage, disk usage, network traffic, (3) Database metrics - query time, connection pool usage, cache hit rate, (4) Business metrics - signups, uploads, challenges completed, XP earned. Monitoring tools: (1) Prometheus for metrics collection, (2) Grafana for dashboards, (3) Sentry for error tracking, (4) CloudWatch/DataDog for cloud monitoring. Alerting rules: (1) Error rate >1% for 5 minutes - page on-call engineer, (2) Response time p95 >2s for 10 minutes - alert team, (3) CPU usage >80% for 15 minutes - alert team, (4) Disk usage >90% - alert team, (5) Service down - page on-call engineer immediately. Alert channels: (1) PagerDuty for critical alerts (24/7 on-call), (2) Slack for warnings, (3) Email for informational. Dashboard views: (1) Overview - key metrics at a glance, (2) Application - detailed application metrics, (3) Infrastructure - server health, (4) Business - user activity and growth. Benefits: (1) Early problem detection (fix before users notice), (2) Faster incident response (alerts notify team), (3) Performance insights (identify bottlenecks), (4) Capacity planning (predict when to scale).

## 4. Data Requirements

### 4.1 Data Models

**DR-4.1.1: User Data Model**
The system shall store user account information in PostgreSQL with the following schema: users table with columns: id (UUID primary key), email (varchar 100, unique, not null), password_hash (varchar 255, not null), username (varchar 50, unique, not null), display_name (varchar 100), avatar_url (varchar 255), created_at (timestamp, not null), updated_at (timestamp, not null), last_login (timestamp), email_verified (boolean, default false), is_active (boolean, default true), preferences (jsonb for settings). Indexes: email (unique), username (unique), created_at (for sorting). Relationships: one-to-many with projects, one-to-many with flashcards, one-to-many with achievements. Data retention: user data retained indefinitely unless user requests deletion (GDPR right to be forgotten). Backup: daily full backup, 30-day retention. This model supports authentication, profile management, and personalization.


**DR-4.1.2: Project Data Model**
The system shall store project metadata in PostgreSQL with the following schema: projects table with columns: id (UUID primary key), user_id (UUID foreign key to users, not null), name (varchar 100, not null), description (text), language (varchar 20), file_count (integer), entity_count (integer), created_at (timestamp, not null), updated_at (timestamp, not null), status (enum: pending/processing/completed/failed), processing_progress (integer 0-100). Indexes: user_id (for user's projects), created_at (for sorting), status (for filtering). Relationships: many-to-one with users, one-to-many with upload_sessions. Data retention: project metadata retained for 1 year after last access, then archived. Backup: daily full backup, 90-day retention. This model tracks project uploads and processing status.

**DR-4.1.3: Code Entity Data Model (Neo4j)**
The system shall store code entities as nodes in Neo4j with labels: File, Function, Class, Variable, Import. Common properties: id (unique identifier), project_id (for isolation), name (entity name), file_path (source file), start_line (integer), end_line (integer), created_at (timestamp). Function-specific properties: signature (function signature), docstring (documentation), body (first 500 chars), is_async (boolean), decorators (array). Class-specific properties: methods (array of method names), base_classes (array of parent class names). Indexes: id (unique), project_id (for filtering), name (for search). Relationships: CALLS (function to function), IMPORTS (file to module), DEFINES (file to entity), EXTENDS (class to class), IMPLEMENTS (class to interface), USES (function to variable). Data retention: entities retained while project exists, deleted when project deleted. Backup: daily full backup, 30-day retention. This model enables graph queries and dependency analysis.

**DR-4.1.4: Vector Embedding Data Model (Chroma)**
The system shall store vector embeddings in Chroma collections with structure: collection name: project_{project_id}_embeddings, embedding: 768-dimensional float array, metadata: {id: entity_id, entity_type: function/class, name: entity_name, file_path: source_file, project_id: project_id, signature: function_signature}. Distance metric: cosine similarity (0-1, higher is more similar). Index: HNSW (Hierarchical Navigable Small World) for fast approximate nearest neighbor search. Data retention: embeddings retained while project exists, deleted when project deleted. Backup: daily backup of collection metadata and vectors, 30-day retention. This model enables semantic search and similar entity discovery.

**DR-4.1.5: Flashcard Data Model**
The system shall store flashcards in PostgreSQL with the following schema: flashcards table with columns: id (UUID primary key), user_id (UUID foreign key, not null), type (enum: basic/cloze/code), front (text, not null), back (text, not null), language (varchar 20 for code cards), tags (array of varchar), source_type (enum: manual/chat/dojo), source_id (UUID reference to source), created_at (timestamp, not null), interval (integer days, default 1), repetitions (integer, default 0), ease_factor (float, default 2.5), next_review (timestamp, not null), last_review (timestamp). Indexes: user_id (for user's cards), next_review (for due cards), source_type (for filtering). Relationships: many-to-one with users. Data retention: cards retained indefinitely unless user deletes. Backup: daily full backup, 90-day retention. This model implements SM-2 spaced repetition algorithm.

**DR-4.1.6: Gamification Data Model**
The system shall store gamification data in PostgreSQL with the following schema: user_stats table with columns: user_id (UUID primary key foreign key), total_xp (integer, default 0), current_league (enum: bronze/silver/gold/platinum/diamond), weekly_xp (integer, default 0), current_streak (integer, default 0), longest_streak (integer, default 0), skill_algorithms (integer 0-100), skill_data_structures (integer 0-100), skill_debugging (integer 0-100), skill_patterns (integer 0-100), skill_testing (integer 0-100), skill_architecture (integer 0-100), updated_at (timestamp). achievements table with columns: id (UUID primary key), user_id (UUID foreign key), achievement_id (varchar 50), unlocked_at (timestamp, not null). daily_quests table with columns: id (UUID primary key), user_id (UUID foreign key), quest_type (varchar 50), progress (integer), target (integer), completed (boolean), claimed (boolean), date (date, not null). Indexes: user_id on all tables, achievement_id (for checking unlocks), date (for daily quests). Data retention: stats retained indefinitely, quests retained for 30 days. Backup: daily full backup, 90-day retention. This model tracks user progress and achievements.

### 4.2 Data Storage

**DR-4.2.1: PostgreSQL for Relational Data**
The system shall use PostgreSQL 14+ for storing relational data (users, projects, flashcards, gamification). Database configuration: (1) Connection pool size: 50 connections, (2) Max connections: 200, (3) Shared buffers: 25% of RAM, (4) Effective cache size: 75% of RAM, (5) Work mem: 16MB per connection, (6) Maintenance work mem: 256MB. Performance optimization: (1) Indexes on foreign keys and frequently queried columns, (2) Partial indexes for filtered queries, (3) VACUUM ANALYZE daily for statistics, (4) Query plan analysis for slow queries (EXPLAIN ANALYZE). High availability: (1) Primary-replica replication (streaming replication), (2) Automatic failover with Patroni or similar, (3) Read replicas for read-heavy queries. Backup: (1) Continuous WAL archiving to S3, (2) Daily full backup, (3) Point-in-time recovery supported, (4) Test restore monthly. PostgreSQL provides: ACID transactions, complex queries, JSON support (jsonb), full-text search, and mature ecosystem.

**DR-4.2.2: Neo4j for Graph Data**
The system shall use Neo4j 5.14+ for storing code entities and relationships. Database configuration: (1) Heap size: 8GB initial, 16GB max, (2) Page cache: 10GB (for graph data), (3) Transaction logs: 2GB, (4) Bolt connection pool: 50 connections. Performance optimization: (1) Indexes on id, project_id, name properties, (2) Constraint on id (unique), (3) Query optimization (use MATCH instead of WHERE, limit result sets), (4) Periodic compaction (remove deleted nodes/relationships). High availability: (1) Causal Cluster with 3 core servers, (2) Read replicas for read-heavy queries, (3) Automatic leader election on failure. Backup: (1) Daily full backup at 2 AM UTC, (2) Incremental backups every 6 hours, (3) Backup to S3, (4) 7 daily + 4 weekly + 3 monthly retention. Neo4j provides: native graph storage, Cypher query language, ACID transactions, and optimized graph algorithms.

**DR-4.2.3: Chroma for Vector Data**
The system shall use Chroma 0.4+ for storing vector embeddings. Database configuration: (1) Persistence: enabled (data stored on disk), (2) Collection per project (project_{project_id}_embeddings), (3) Distance metric: cosine similarity, (4) Index: HNSW (M=16, ef_construction=200), (5) Embedding dimension: 768 (Gemini text-embedding-004). Performance optimization: (1) Batch operations (insert/query 100 vectors at once), (2) Metadata filtering before vector search (reduces search space), (3) Connection pooling (20 connections), (4) Query result caching in Redis. High availability: (1) Multiple Chroma instances behind load balancer, (2) Partition collections by project_id (consistent hashing), (3) Replicate collections across instances. Backup: (1) Daily backup of collection metadata and vectors, (2) Backup to S3, (3) 7-day retention. Chroma provides: fast similarity search, metadata filtering, and easy deployment.

**DR-4.2.4: Redis for Caching**
The system shall use Redis 7+ for caching query results and session data. Cache configuration: (1) Max memory: 4GB, (2) Eviction policy: allkeys-lru (least recently used), (3) Persistence: RDB snapshots every 15 minutes, (4) AOF (Append-Only File): disabled (cache data is not critical). Cache keys: (1) Query results: "query:{type}:{hash}:{project_id}" with 5-minute TTL, (2) Session data: "session:{session_id}" with 7-day TTL, (3) Rate limits: "ratelimit:{user_id}:{window}" with 1-hour TTL, (4) Circuit breaker: "circuit:{service}" with 1-minute TTL. Cache invalidation: (1) Time-based (TTL expires), (2) Event-based (delete on project update), (3) Manual (admin can flush cache). High availability: (1) Redis Cluster with 3+ nodes, (2) Automatic failover with Redis Sentinel, (3) Replication for read scaling. Backup: (1) RDB snapshots to S3 every hour, (2) 24-hour retention (cache data is not critical). Redis provides: sub-millisecond latency, atomic operations, and pub/sub for real-time features.

**DR-4.2.5: S3 for File Storage**
The system shall use S3 (or compatible object storage) for storing uploaded files and backups. Storage structure: (1) Uploads: s3://bucket/uploads/{user_id}/{project_id}/{file_path}, (2) Backups: s3://bucket/backups/{service}/{date}/{backup_file}, (3) Static assets: s3://bucket/static/{version}/{asset}. Storage configuration: (1) Versioning enabled (keep old versions for recovery), (2) Lifecycle policies (move old versions to Glacier after 30 days), (3) Encryption at rest (AES-256), (4) Access control (IAM policies, bucket policies). Performance optimization: (1) CloudFront CDN for static assets, (2) Multipart upload for large files (>100MB), (3) Transfer acceleration for global uploads, (4) Parallel downloads for faster retrieval. High availability: (1) S3 provides 99.99% availability, (2) Cross-region replication for disaster recovery, (3) Versioning prevents accidental deletion. Backup: (1) S3 versioning serves as backup, (2) Cross-region replication to second region, (3) Lifecycle policies for cost optimization. S3 provides: unlimited storage, high durability (99.999999999%), and global accessibility.

### 4.3 Data Retention

**DR-4.3.1: User Data Retention**
The system shall retain user data according to the following policies: (1) Active users - data retained indefinitely while account is active, (2) Inactive users - data retained for 2 years after last login, then account marked for deletion, (3) Deleted accounts - data soft-deleted (marked as deleted, not physically removed) for 30 days to allow recovery, then permanently deleted, (4) GDPR requests - user can request data export (JSON format) or deletion (processed within 30 days). Data export includes: profile information, projects, flashcards, achievements, activity history. Data deletion includes: user record, projects, flashcards, achievements, uploaded files, but retains anonymized analytics (user_id replaced with random ID). Compliance: GDPR (EU), CCPA (California), PIPEDA (Canada). This policy balances user privacy rights with business needs and legal requirements.


**DR-4.3.2: Project Data Retention**
The system shall retain project data according to the following policies: (1) Active projects - data retained indefinitely while user accesses project (last_accessed updated on each access), (2) Inactive projects - data retained for 1 year after last access, then archived (moved to cold storage), (3) Archived projects - data retained in cold storage for 2 years, then permanently deleted, (4) User-deleted projects - data soft-deleted for 30 days (allow recovery), then permanently deleted. Archival process: (1) Export project data to JSON, (2) Compress and upload to Glacier, (3) Delete from hot databases (Neo4j, Chroma), (4) Keep metadata in PostgreSQL with archived flag. Restoration process: (1) User requests restoration, (2) Download from Glacier (3-5 hours), (3) Decompress and import to databases, (4) Mark as active. This policy reduces storage costs while preserving user data for reasonable period.

**DR-4.3.3: Log Data Retention**
The system shall retain log data according to the following policies: (1) Application logs - retained for 30 days in hot storage (Elasticsearch, CloudWatch), then deleted, (2) Access logs - retained for 90 days in hot storage, then archived to S3 for 1 year, then deleted, (3) Error logs - retained for 90 days in hot storage (Sentry), then deleted, (4) Audit logs - retained for 7 years in cold storage (compliance requirement). Log aggregation: (1) Collect logs from all services, (2) Parse and structure logs, (3) Index for searching, (4) Alert on error patterns. Log analysis: (1) Error rate trends, (2) Slow query identification, (3) User behavior patterns, (4) Security incident detection. This policy balances debugging needs, compliance requirements, and storage costs.

**DR-4.3.4: Backup Data Retention**
The system shall retain backup data according to the following policies: (1) Daily backups - retained for 7 days, (2) Weekly backups - retained for 4 weeks, (3) Monthly backups - retained for 3 months, (4) Yearly backups - retained for 2 years. Backup types: (1) Database backups (PostgreSQL, Neo4j, Chroma), (2) File backups (S3 versioning), (3) Configuration backups (environment variables, secrets). Backup testing: (1) Test restore monthly to staging environment, (2) Verify data integrity, (3) Measure restore time (RTO), (4) Document restore procedures. Backup storage: (1) Primary backups in same region as production, (2) Cross-region replication for disaster recovery, (3) Encryption at rest and in transit. This policy ensures data can be recovered from various failure scenarios while managing storage costs.

## 5. Integration Requirements

### 5.1 External APIs

**IR-5.1.1: Google Gemini API Integration**
The system shall integrate with Google Gemini API for AI completions and embeddings. API endpoints: (1) generateContent - for chat completions (Learning/Building Mode), (2) generateContentStream - for streaming responses, (3) embedContent - for generating 768-dimensional embeddings. API configuration: (1) Model: gemini-2.0-flash-exp for completions, text-embedding-004 for embeddings, (2) API key: stored in environment variable GEMINI_API_KEY, (3) Rate limits: 60 requests/minute (completions), 1500 requests/minute (embeddings), (4) Timeout: 30s for completions, 10s for embeddings, (5) Retry: 3 attempts with exponential backoff. Error handling: (1) Rate limit (429) - wait and retry with backoff, (2) Server error (5xx) - retry with backoff, (3) Invalid request (4xx) - return error to user, (4) Timeout - return error with retry option. Cost optimization: (1) Cache common queries in Redis, (2) Batch embedding generation (10 per request), (3) Use streaming for long responses (better UX, same cost). This integration provides the AI capabilities that differentiate SocraticDev from traditional learning platforms.

**IR-5.1.2: OAuth Provider Integration**
The system shall integrate with OAuth 2.0 providers for social authentication. Supported providers: (1) Google OAuth 2.0, (2) GitHub OAuth 2.0. OAuth flow: (1) User clicks "Sign in with Google/GitHub", (2) Redirect to provider's authorization page, (3) User grants permissions, (4) Provider redirects back with authorization code, (5) Exchange code for access token, (6) Retrieve user profile (email, name, avatar), (7) Create or link account based on email, (8) Generate JWT and return to client. OAuth configuration: (1) Client ID and secret stored in environment variables, (2) Redirect URI: https://socraticdev.com/auth/callback/{provider}, (3) Scopes: email, profile (Google), user:email (GitHub), (4) State parameter for CSRF protection. Error handling: (1) User denies permission - show error message, (2) Invalid code - retry authorization, (3) Email already exists - offer to link accounts. This integration provides convenient authentication and reduces password management burden.

**IR-5.1.3: Email Service Integration**
The system shall integrate with email service for transactional emails. Email provider: SendGrid, AWS SES, or similar. Email types: (1) Verification email - sent on registration with verification link, (2) Password reset - sent on password reset request with reset link, (3) Welcome email - sent after email verification, (4) Achievement notifications - sent when user unlocks rare/epic/legendary achievements (optional, user can disable), (5) Streak reminders - sent at 8 PM if user hasn't learned today (optional, user can disable). Email configuration: (1) API key stored in environment variable, (2) From address: noreply@socraticdev.com, (3) Reply-to address: support@socraticdev.com, (4) Rate limit: 100 emails/minute. Email templates: (1) HTML templates with responsive design, (2) Plain text fallback, (3) Personalization (user name, dynamic content), (4) Unsubscribe link (required by law). Error handling: (1) Invalid email - return error to user, (2) Rate limit - queue and retry, (3) Bounce - mark email as invalid, (4) Spam complaint - unsubscribe user. This integration enables essential user communication and engagement.

### 5.2 Database Integrations

**IR-5.2.1: Neo4j Driver Integration**
The system shall integrate with Neo4j using the official Python driver. Driver configuration: (1) URI: neo4j://localhost:7687 (development), neo4j+s://production-host:7687 (production with TLS), (2) Authentication: username and password from environment variables, (3) Connection pool: 50 connections, (4) Max connection lifetime: 1 hour, (5) Connection timeout: 30s, (6) Max retry time: 30s. Query execution: (1) Use parameterized queries (prevent injection), (2) Use transactions for multi-query operations, (3) Use read transactions for read-only queries (can use replicas), (4) Use write transactions for write queries (must use leader). Error handling: (1) ServiceUnavailable - retry with backoff, (2) TransientError - retry immediately, (3) ClientError - return error to user, (4) Timeout - return error with retry option. Connection management: (1) Create driver on application startup, (2) Close driver on application shutdown, (3) Verify connectivity with health check, (4) Reconnect automatically on connection loss. This integration provides the graph database capabilities essential for code structure analysis.

**IR-5.2.2: Chroma Client Integration**
The system shall integrate with Chroma using the official Python client. Client configuration: (1) Host: localhost:8000 (development), production-host:8000 (production), (2) Connection pool: 20 connections, (3) Timeout: 10s for queries, 60s for batch operations, (4) Retry: 3 attempts with exponential backoff. Collection management: (1) Create collection per project (project_{project_id}_embeddings), (2) Set distance metric to cosine, (3) Set embedding dimension to 768, (4) Create HNSW index (M=16, ef_construction=200). Operations: (1) Add embeddings in batches (100 per batch), (2) Query with metadata filtering (filter by project_id, entity_type), (3) Delete collection when project deleted, (4) Get collection statistics (count, size). Error handling: (1) Collection not found - create collection, (2) Connection error - retry with backoff, (3) Timeout - return error with retry option, (4) Invalid embedding dimension - return error to user. This integration provides the semantic search capabilities that enable context-aware AI assistance.

**IR-5.2.3: Redis Client Integration**
The system shall integrate with Redis using the redis-py library. Client configuration: (1) Host: localhost:6379 (development), production-host:6379 (production), (2) Connection pool: 10 connections, (3) Timeout: 5s, (4) Retry: 3 attempts with exponential backoff, (5) Decode responses: True (return strings, not bytes). Operations: (1) GET/SET for simple caching, (2) SETEX for caching with TTL, (3) INCR for rate limiting counters, (4) EXPIRE for setting TTL on existing keys, (5) DEL for cache invalidation, (6) KEYS for pattern matching (use sparingly, O(n) operation). Error handling: (1) Connection error - continue without cache (degrade gracefully), (2) Timeout - continue without cache, (3) Memory full - evict LRU keys automatically. Connection management: (1) Create connection pool on startup, (2) Close pool on shutdown, (3) Verify connectivity with PING, (4) Reconnect automatically on connection loss. This integration provides caching that significantly improves performance and reduces database load.

## 6. Future Enhancements (AWS Integration Roadmap)

### 6.1 AWS Bedrock Integration

**FE-6.1.1: AWS Bedrock for AI Models**
Future enhancement: Migrate from Google Gemini to AWS Bedrock for AI completions and embeddings. Benefits: (1) Multi-model support (Claude, Llama, Titan), (2) Lower latency (AWS infrastructure), (3) Better cost control (reserved capacity), (4) Unified AWS ecosystem. Implementation: (1) Replace Gemini API calls with Bedrock API calls, (2) Use Claude 3 Sonnet for completions (similar quality to Gemini), (3) Use Titan Embeddings for vector generation (768 dimensions), (4) Implement model switching (allow users to choose model). Migration strategy: (1) Implement Bedrock integration alongside Gemini, (2) A/B test with subset of users, (3) Compare quality, latency, and cost, (4) Gradually migrate all users, (5) Deprecate Gemini integration. Timeline: Q3 2026 (6 months after launch).

**FE-6.1.2: AWS Bedrock Knowledge Bases**
Future enhancement: Use AWS Bedrock Knowledge Bases for RAG instead of custom GraphRAG implementation. Benefits: (1) Managed vector database (no Chroma maintenance), (2) Built-in chunking and embedding, (3) Integrated with Bedrock models, (4) Automatic scaling. Implementation: (1) Create Knowledge Base per project, (2) Upload code files to S3, (3) Knowledge Base automatically chunks and embeds, (4) Query Knowledge Base with natural language, (5) Bedrock retrieves relevant chunks and generates response. Migration strategy: (1) Implement Knowledge Bases for new projects, (2) Migrate existing projects gradually, (3) Compare quality with custom GraphRAG, (4) Keep graph database for relationship queries. Timeline: Q4 2026 (9 months after launch).


### 6.2 AWS Infrastructure

**FE-6.2.1: AWS ECS for Container Orchestration**
Future enhancement: Deploy containers on AWS ECS (Elastic Container Service) instead of manual Docker deployment. Benefits: (1) Automatic scaling based on load, (2) Load balancing across containers, (3) Health checks and automatic replacement, (4) Integration with AWS services. Implementation: (1) Create ECS cluster, (2) Define task definitions for each service (frontend, backend, celery), (3) Create services with desired count and auto-scaling policies, (4) Use Application Load Balancer for traffic distribution, (5) Use CloudWatch for monitoring. Auto-scaling policies: (1) Scale up when CPU >70% for 5 minutes, (2) Scale down when CPU <30% for 10 minutes, (3) Min 2 instances, max 10 instances per service. Timeline: Q2 2026 (3 months after launch).

**FE-6.2.2: AWS RDS for PostgreSQL**
Future enhancement: Migrate PostgreSQL to AWS RDS (Relational Database Service) for managed database. Benefits: (1) Automated backups and point-in-time recovery, (2) Automatic failover with Multi-AZ deployment, (3) Read replicas for scaling, (4) Automated patching and maintenance. Implementation: (1) Create RDS PostgreSQL instance (db.r6g.xlarge), (2) Enable Multi-AZ for high availability, (3) Create read replicas for read-heavy queries, (4) Configure automated backups (daily, 30-day retention), (5) Enable encryption at rest. Migration strategy: (1) Create RDS instance, (2) Replicate data from self-hosted PostgreSQL, (3) Switch application to RDS, (4) Monitor for issues, (5) Decommission self-hosted database. Timeline: Q2 2026 (3 months after launch).

**FE-6.2.3: AWS ElastiCache for Redis**
Future enhancement: Migrate Redis to AWS ElastiCache for managed caching. Benefits: (1) Automatic failover with Redis Cluster, (2) Automatic backups, (3) Automatic patching, (4) Better performance with enhanced I/O. Implementation: (1) Create ElastiCache Redis cluster (cache.r6g.large), (2) Enable cluster mode for sharding, (3) Configure automatic backups (daily), (4) Enable encryption in transit and at rest. Migration strategy: (1) Create ElastiCache cluster, (2) Update application to use ElastiCache endpoint, (3) Warm cache by replicating data, (4) Switch traffic to ElastiCache, (5) Decommission self-hosted Redis. Timeline: Q2 2026 (3 months after launch).

**FE-6.2.4: AWS S3 and CloudFront**
Future enhancement: Already using S3 for file storage, add CloudFront CDN for global distribution. Benefits: (1) Lower latency for global users (edge locations), (2) Reduced S3 costs (CloudFront caching), (3) DDoS protection, (4) HTTPS by default. Implementation: (1) Create CloudFront distribution, (2) Set S3 bucket as origin, (3) Configure caching policies (1 year for versioned assets), (4) Enable compression (gzip, brotli), (5) Configure custom domain (cdn.socraticdev.com). Cache invalidation: (1) Use versioned filenames (main.abc123.js) for automatic cache busting, (2) Invalidate manually for non-versioned files (index.html). Timeline: Q1 2026 (immediately after launch).

### 6.3 AWS AI Services

**FE-6.3.1: AWS CodeWhisperer Integration**
Future enhancement: Integrate AWS CodeWhisperer for code completion in Monaco Editor. Benefits: (1) Real-time code suggestions, (2) Context-aware completions, (3) Security scanning, (4) Best practice recommendations. Implementation: (1) Add CodeWhisperer SDK to frontend, (2) Configure API credentials, (3) Hook into Monaco Editor's completion provider, (4) Show suggestions as user types, (5) Track acceptance rate. User experience: (1) Suggestions appear as gray text (ghost text), (2) Tab to accept, Esc to dismiss, (3) Multiple suggestions available (cycle with Alt+]), (4) Disable option in settings. Timeline: Q4 2026 (9 months after launch).

**FE-6.3.2: AWS Comprehend for Code Analysis**
Future enhancement: Use AWS Comprehend for natural language analysis of code comments and documentation. Benefits: (1) Extract key phrases from docstrings, (2) Sentiment analysis of code comments, (3) Entity recognition (function names, variable names), (4) Language detection for multilingual codebases. Implementation: (1) Send docstrings and comments to Comprehend, (2) Extract key phrases for search indexing, (3) Identify important concepts for flashcard generation, (4) Detect code quality issues from comment sentiment. Use cases: (1) Better semantic search (index key phrases), (2) Automatic flashcard generation (key concepts), (3) Code quality insights (negative sentiment in comments indicates problems). Timeline: Q1 2027 (12 months after launch).

**FE-6.3.3: AWS Rekognition for Diagram Analysis**
Future enhancement: Use AWS Rekognition for analyzing architecture diagrams and flowcharts in documentation. Benefits: (1) Extract text from diagrams (OCR), (2) Detect diagram type (flowchart, UML, architecture), (3) Identify components and relationships, (4) Generate graph representation. Implementation: (1) Detect images in documentation, (2) Send to Rekognition for analysis, (3) Extract text and shapes, (4) Convert to graph structure, (5) Store in Neo4j alongside code entities. Use cases: (1) Understand architecture from diagrams, (2) Link diagrams to code entities, (3) Verify code matches architecture, (4) Generate documentation from code. Timeline: Q2 2027 (15 months after launch).

## 7. Constraints and Assumptions

### 7.1 Technical Constraints

**C-7.1.1: Browser Compatibility**
The system must support modern browsers: Chrome 90+, Firefox 88+, Safari 14+, Edge 90+. No support for Internet Explorer (end-of-life). Assumption: Users have JavaScript enabled (required for React). Assumption: Users have cookies enabled (required for authentication). Constraint: Some features require modern APIs (WebGL for 3D visualizations, WebAssembly for performance). Fallback: Detect unsupported browsers and show upgrade message.

**C-7.1.2: API Rate Limits**
The system is constrained by external API rate limits: (1) Gemini API - 60 requests/minute for completions, 1500 requests/minute for embeddings, (2) GitHub API - 5000 requests/hour for authenticated requests (OAuth), (3) Google OAuth - no published limit but subject to abuse detection. Mitigation: (1) Implement caching to reduce API calls, (2) Implement rate limiting on our side to stay within limits, (3) Queue requests when approaching limits, (4) Show user-friendly error messages when limits exceeded.

**C-7.1.3: File Size Limits**
The system is constrained by file size limits: (1) Individual file upload - 10MB max (prevent memory issues during parsing), (2) Total project size - 1GB max (prevent storage issues), (3) Request body size - 100MB max (FastAPI limit), (4) Response size - 10MB max (prevent timeout). Assumption: Most code files are <1MB (typical source files are 10-100KB). Mitigation: (1) Validate file sizes before upload, (2) Compress large files, (3) Stream large responses, (4) Paginate large result sets.

**C-7.1.4: Processing Time Limits**
The system is constrained by processing time limits: (1) API request timeout - 30s (prevent hanging requests), (2) Celery task timeout - 1 hour (prevent runaway tasks), (3) Database query timeout - 5s (prevent slow queries), (4) AI completion timeout - 30s (prevent long waits). Assumption: Most operations complete within limits (typical upload processes 100 files/minute). Mitigation: (1) Show progress indicators for long operations, (2) Allow cancellation of long operations, (3) Optimize slow queries, (4) Use streaming for long AI responses.

### 7.2 Business Constraints

**C-7.2.1: Cost Constraints**
The system must operate within budget constraints: (1) AI API costs - $0.01 per 1000 tokens (Gemini), target <$1000/month for 10,000 users, (2) Infrastructure costs - target <$500/month for hosting (AWS, databases), (3) Total operating costs - target <$2000/month. Assumption: Average user makes 50 AI queries/month (50,000 tokens = $0.50/user/month). Mitigation: (1) Implement caching to reduce API calls, (2) Use cost-effective infrastructure (spot instances, reserved capacity), (3) Monitor costs daily, (4) Implement usage limits per user.

**C-7.2.2: Development Timeline**
The system must launch within 6 months (by July 2026). Constraint: Limited development team (4 developers). Assumption: Core features can be implemented in 6 months with focused effort. Mitigation: (1) Prioritize MVP features (chat, upload, dojo, SRS), (2) Defer advanced features (code visualizer, advanced analytics), (3) Use existing libraries and services (don't build from scratch), (4) Implement iteratively (release early, iterate based on feedback).

**C-7.2.3: Compliance Requirements**
The system must comply with data protection regulations: (1) GDPR (EU) - user consent, data portability, right to be forgotten, (2) CCPA (California) - data disclosure, opt-out of sale, (3) COPPA (US) - no users under 13 without parental consent. Assumption: Most users are 18+ (college students, professionals). Mitigation: (1) Implement age verification (require birthdate on signup), (2) Implement data export and deletion, (3) Implement cookie consent banner, (4) Maintain privacy policy and terms of service.

### 7.3 Assumptions

**A-7.3.1: User Assumptions**
Assumption: Users have basic programming knowledge (understand variables, functions, loops). Assumption: Users have access to modern devices (laptop or desktop with 4GB+ RAM, mobile device with 2GB+ RAM). Assumption: Users have stable internet connection (minimum 1 Mbps for basic functionality, 5 Mbps for video content). Assumption: Users are motivated to learn (willing to spend 30+ minutes per session). Validation: User research, beta testing, analytics tracking.

**A-7.3.2: Technical Assumptions**
Assumption: Tree-sitter parsers accurately parse code (>95% accuracy for supported languages). Assumption: Gemini API provides high-quality responses (comparable to GPT-4). Assumption: Neo4j can handle graphs with millions of nodes (tested up to 10M nodes). Assumption: Chroma can handle millions of vectors (tested up to 5M vectors). Validation: Load testing, performance benchmarking, user feedback.

**A-7.3.3: Business Assumptions**
Assumption: Developers want to learn through Socratic method (not just get answers). Assumption: Users will pay for premium features (advanced analytics, unlimited uploads, priority support). Assumption: Market size is sufficient (10M+ developers learning to code globally). Assumption: Competition is not insurmountable (differentiation through Socratic method and GraphRAG). Validation: Market research, user interviews, competitor analysis, beta testing.

## 8. Success Metrics

### 8.1 User Engagement Metrics

**M-8.1.1: Daily Active Users (DAU)**
Target: 1,000 DAU within 3 months of launch, 5,000 DAU within 6 months. Measurement: Count unique users who perform at least one action (chat query, challenge completion, flashcard review) per day. Tracking: Google Analytics, Mixpanel, or custom analytics. Benchmark: 20% DAU/MAU ratio (industry standard for learning platforms). Action: If below target, improve onboarding, add daily quests, send reminder notifications.

**M-8.1.2: Session Duration**
Target: Average session duration of 25 minutes. Measurement: Time from first action to last action in a session (session ends after 30 minutes of inactivity). Tracking: Custom analytics with session tracking. Benchmark: 20-30 minutes is typical for learning platforms. Action: If below target, improve content engagement, add more interactive features, reduce friction.


**M-8.1.3: Feature Adoption Rate**
Target: 70% of users try at least 3 different features (chat, dojo, SRS, graph, visualizer) within first week. Measurement: Track feature usage per user, calculate percentage who use 3+ features. Tracking: Custom analytics with feature flags. Benchmark: 60-80% multi-feature adoption for successful platforms. Action: If below target, improve feature discovery, add guided tours, highlight features in onboarding.

**M-8.1.4: Retention Rate**
Target: 40% Day 7 retention, 25% Day 30 retention. Measurement: Percentage of users who return on Day 7/30 after signup. Tracking: Cohort analysis in analytics platform. Benchmark: 30-50% D7 retention, 20-30% D30 retention for learning platforms. Action: If below target, improve onboarding, add habit-forming features (streaks, daily quests), send re-engagement emails.

### 8.2 Learning Effectiveness Metrics

**M-8.2.1: Challenge Completion Rate**
Target: 60% completion rate for started challenges. Measurement: (Completed challenges / Started challenges) × 100. Tracking: Custom analytics tracking challenge starts and completions. Benchmark: 50-70% completion rate for educational content. Action: If below target, adjust difficulty, provide better hints, improve challenge design.

**M-8.2.2: Flashcard Review Consistency**
Target: 70% of users review flashcards at least 3 times per week. Measurement: Track review sessions per user per week, calculate percentage meeting threshold. Tracking: Custom analytics with SRS data. Benchmark: 60-80% consistency for spaced repetition apps. Action: If below target, send reminder notifications, gamify reviews (XP rewards), improve review experience.

**M-8.2.3: Skill Progression**
Target: Average skill radar score increases by 20 points within 30 days. Measurement: Compare skill radar scores at Day 1 vs. Day 30. Tracking: Custom analytics with skill tracking. Benchmark: 15-25 point increase for active learners. Action: If below target, ensure skill attribution is accurate, provide more skill-building content, give feedback on weak areas.

**M-8.2.4: Learning Mode Usage**
Target: 60% of AI queries in Learning Mode (vs. Building Mode). Measurement: (Learning Mode queries / Total queries) × 100. Tracking: Custom analytics tracking mode per query. Benchmark: 50-70% learning mode for educational platforms. Action: If below target, educate users on benefits of Learning Mode, make Learning Mode default, add incentives (bonus XP).

### 8.3 Technical Performance Metrics

**M-8.3.1: API Response Time**
Target: p95 response time <500ms for graph queries, <1s for semantic search, <2s for AI completions. Measurement: Track response time for each endpoint, calculate 95th percentile. Tracking: Application Performance Monitoring (APM) tool (DataDog, New Relic). Benchmark: <1s for good user experience. Action: If above target, optimize queries, add caching, scale infrastructure.

**M-8.3.2: Upload Processing Rate**
Target: Process 100 files per minute. Measurement: Track files processed per minute during uploads. Tracking: Custom metrics in Celery monitoring. Benchmark: 80-120 files/minute for code parsing. Action: If below target, optimize parsing, add more workers, improve parallelization.

**M-8.3.3: Error Rate**
Target: <0.1% error rate for API requests. Measurement: (Failed requests / Total requests) × 100. Tracking: APM tool, error tracking (Sentry). Benchmark: <0.5% for production systems. Action: If above target, fix bugs, improve error handling, add retries.

**M-8.3.4: Uptime**
Target: 99.9% uptime (43 minutes downtime per month). Measurement: Track service availability with uptime monitoring (Pingdom, UptimeRobot). Tracking: External monitoring service. Benchmark: 99.9% for SaaS applications. Action: If below target, improve infrastructure reliability, add redundancy, implement better monitoring.

### 8.4 Business Metrics

**M-8.4.1: User Growth Rate**
Target: 20% month-over-month growth in signups. Measurement: (New signups this month / New signups last month - 1) × 100. Tracking: Analytics platform. Benchmark: 15-30% MoM growth for early-stage startups. Action: If below target, increase marketing, improve SEO, add referral program, improve landing page conversion.

**M-8.4.2: Conversion Rate**
Target: 5% conversion rate from landing page visit to signup. Measurement: (Signups / Landing page visits) × 100. Tracking: Google Analytics with goal tracking. Benchmark: 2-10% for SaaS landing pages. Action: If below target, A/B test landing page, improve value proposition, reduce friction in signup flow.

**M-8.4.3: Premium Conversion Rate (Future)**
Target: 10% conversion from free to premium within 3 months. Measurement: (Premium signups / Total signups) × 100 for 3-month cohort. Tracking: Custom analytics with subscription tracking. Benchmark: 5-15% for freemium SaaS. Action: If below target, improve premium features, add more value, optimize pricing, improve upgrade prompts.

**M-8.4.4: Net Promoter Score (NPS)**
Target: NPS of 50+ (excellent). Measurement: Survey users "How likely are you to recommend SocraticDev?" (0-10 scale), NPS = % Promoters (9-10) - % Detractors (0-6). Tracking: In-app survey after 2 weeks of usage. Benchmark: 30-50 is good, 50+ is excellent. Action: If below target, identify pain points from detractor feedback, improve product, follow up with detractors.

## 9. Glossary

**AST (Abstract Syntax Tree)**: Tree representation of source code structure, generated by parsing code. Used for code analysis and entity extraction.

**Bcrypt**: Password hashing algorithm that is intentionally slow to prevent brute force attacks. Uses salt and cost factor for security.

**Celery**: Distributed task queue for Python that handles asynchronous background jobs. Used for upload processing.

**Chroma**: Open-source vector database for storing and searching embeddings. Used for semantic code search.

**Circuit Breaker**: Design pattern that prevents cascading failures by stopping calls to failing services. Has three states: closed, open, half-open.

**CORS (Cross-Origin Resource Sharing)**: Security mechanism that allows or restricts resources on a web page to be requested from another domain.

**Cosine Similarity**: Measure of similarity between two vectors, ranging from 0 (completely different) to 1 (identical). Used for semantic search.

**CSRF (Cross-Site Request Forgery)**: Attack where malicious website tricks user into submitting unwanted request to victim site. Prevented with CSRF tokens.

**Embedding**: Dense vector representation of text or code that captures semantic meaning. Generated by AI models, typically 768 dimensions.

**GDPR (General Data Protection Regulation)**: EU regulation on data protection and privacy. Requires user consent, data portability, and right to be forgotten.

**GraphRAG (Graph Retrieval-Augmented Generation)**: Technique that combines graph database (relationships) and vector database (semantics) for context retrieval in AI systems.

**HNSW (Hierarchical Navigable Small World)**: Algorithm for approximate nearest neighbor search in high-dimensional spaces. Used by Chroma for fast vector search.

**JWT (JSON Web Token)**: Compact, URL-safe token format for authentication. Contains user information and signature to prevent tampering.

**Neo4j**: Graph database that stores data as nodes and relationships. Uses Cypher query language. Used for code structure storage.

**OAuth 2.0**: Authorization framework that allows third-party applications to access user data without exposing passwords. Used for Google/GitHub login.

**Parsons Problem**: Educational exercise where students arrange scrambled code blocks in correct order. Reduces cognitive load compared to writing from scratch.

**Redis**: In-memory data store used for caching and session management. Provides sub-millisecond latency.

**SM-2 (SuperMemo 2)**: Spaced repetition algorithm that optimizes review scheduling based on user performance. Used in flashcard system.

**Socratic Method**: Teaching technique that uses questions to guide learners to discover answers themselves. Builds deeper understanding than direct instruction.

**Tree-sitter**: Incremental parsing library that builds syntax trees from source code. Supports multiple languages and handles syntax errors gracefully.

**WCAG (Web Content Accessibility Guidelines)**: International standards for web accessibility. Level AA compliance ensures usability for people with disabilities.

**XSS (Cross-Site Scripting)**: Security vulnerability where attacker injects malicious scripts into web pages. Prevented with output encoding and CSP.

## 10. Document History

**Version 1.0** - January 25, 2026
- Initial comprehensive requirements document
- Covers all functional and non-functional requirements
- Includes data models, integrations, and future enhancements
- Defines success metrics and constraints
- Total: 2000+ lines of detailed requirements

**Document Owner**: Ishan Kumar (Lead Developer)

**Contributors**: 
- Hemant Singh (AI/ML Requirements)
- Mohd. Hassan (UX Requirements)
- Ishu (QA and Testing Requirements)

**Review Cycle**: Quarterly review and updates based on user feedback and technical evolution

**Next Review**: April 25, 2026

---

**END OF REQUIREMENTS DOCUMENT**

This comprehensive requirements document covers all aspects of the SocraticDev platform, from user-facing features to technical infrastructure. Each requirement is explained in detail with purpose, implementation approach, user experience considerations, technical details, and business value. The document serves as the single source of truth for development, ensuring all team members understand what needs to be built and why.